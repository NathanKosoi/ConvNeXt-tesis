{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8846f0e1",
   "metadata": {},
   "source": [
    "# Modelo ConvNeXt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b77232",
   "metadata": {},
   "source": [
    "En este notebook explicaremos el proceso para construir una red neuronal convolucional *ConvNeXt* a partir de una red neuronal *ResNet*. Nos basaremos en el artículo [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545), donde fueron introducidas las redes *ConvNeXt*.\n",
    "\n",
    "El proceso consiste en siete pasos:\n",
    "\n",
    "1. **Crear un modelo base de ResNet basado en ResNet-50**\n",
    "2. **Cambiar la proporción de los stages del modelo ResNet**\n",
    "3. **Cambiar el bloque *Stem* por un bloque *Patchify***\n",
    "4. **Añadir *depthwise convolution* (*ResNeXt-ify*)**\n",
    "5. **Cambiar el bloque *Bottleneck* por *Inverted-Bottleneck***\n",
    "6. **Aumentar el tamaño de los kernels**\n",
    "7. **Cambios en el *microdiseño***\n",
    "\n",
    "\n",
    "Para medir los cambios en la exactitud (*accuracy*) del modelo en cada uno de los siete pasos, entrenaremos al correspondiente modelo por 100 épocas utilizando los mismos parámetros de entrenamiento. Repetiremos tres veces cada experimento y reportaremos el promedio de los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d3cc99",
   "metadata": {},
   "source": [
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2367153",
   "metadata": {},
   "source": [
    "Entrenaremos al modelo en el conjunto de datos CIFAR-10. Este dataset consiste de 60000 imágenes a color en 10 clases distintas, donde no hay intersección entre las distintas clases. Se puede acceder al dataset mediante las herramientas de la paquetería de pytorch, o también en la página oficial: https://www.cs.toronto.edu/~kriz/cifar.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dc37dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las paqueterías necesarias para el notebook\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import StochasticDepth\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# De ser posible utilizaremos GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96d53aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def data_loader(data_dir,\n",
    "                batch_size,\n",
    "                random_seed=42,\n",
    "                valid_size=0.1,\n",
    "                shuffle=False,\n",
    "                test=False):\n",
    "    \"\"\"\n",
    "    Función para cargar los datos de CIFAR-10\n",
    "    \"\"\"\n",
    "    \n",
    "    # Definimos el transform para normalizar los datos con pytorch\n",
    "    # Los valores fueron obtenidos en el notebook \"data_extraction.ipynb\"\n",
    "    normalize = transforms.Normalize(  \n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # Definimos el transform para preporcesar los datos\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    # Obtener los datos del conjunto de prueba\n",
    "    if test:\n",
    "        dataset = datasets.CIFAR10(\n",
    "          root=data_dir, train=False,\n",
    "          download=True, transform=transform_test,\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    # Cargamos una copia de los datos de entrenamiento\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform_train,\n",
    "    )\n",
    "    \n",
    "    # Cargamos una copia extra de los datos de entrenamiento para dividirlo después en el conjunto de validación\n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform_train,\n",
    "    )\n",
    "    \n",
    "    # Separamos los datos de entrenamiento y validación mediante índices\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    # Finalmente, definimos los conjuntos de entrenamiento y validación\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "\n",
    "# Aplicamos la función para cargar los datos de CIFAR-10, los guardamos en el directorio actual\n",
    "train_loader, valid_loader = data_loader(data_dir='./data',\n",
    "                                         batch_size=64)\n",
    "\n",
    "test_loader = data_loader(data_dir='./data',\n",
    "                              batch_size=64,\n",
    "                              test=True)    \n",
    "cifar10_classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f1a31",
   "metadata": {},
   "source": [
    "### Función de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ab333",
   "metadata": {},
   "source": [
    "Con la siguiente función obtendremos los datos del desempeño de cada modelo a lo largo del entrenamiento. Los parámetros de entrenamiento son los mismo que los utilizados en el artículo donde fueorn introducidas las redes [*ConvNeXt*](https://arxiv.org/abs/2201.03545), con la excepción del número de épocas, el cual no demostró mejorar el desempeño de la red al ser aumentado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4777d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenamiento(model, epocas):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # variables para guardar los resultados\n",
    "    accuracy_training_epochs = []\n",
    "    accuracy_validation_epochs = []\n",
    "    loss_epoch = []\n",
    "    test_accuracy = []\n",
    "    best_model = None\n",
    "    \n",
    "    # parámetros de entrenamiento\n",
    "    num_epochs = epocas\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(),\n",
    "                            lr=0.004,\n",
    "                            betas=(0.9, 0.999),\n",
    "                            weight_decay=0.05\n",
    "                            )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epocas)\n",
    "\n",
    "    \n",
    "    # entrenamiento\n",
    "    print(\"Comenzando entrenamiento\")\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "            # Mover a los tensores a GPU de ser posible\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Ahorro de memoria\n",
    "            del images, labels, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        loss_epoch.append(loss.item()) # Guardar la información del loss de esta época\n",
    "        lr_scheduler.step() # Implementación de learning rate decay\n",
    "\n",
    "        # Medición de la exactitud en el conjunto de validación\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in valid_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                del images, labels, outputs\n",
    "            val_accuracy = correct/total\n",
    "            accuracy_validation_epochs.append(val_accuracy)\n",
    "\n",
    "        # Medición de la exactitud sobre todo el conjunto de entrenamiento\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in train_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                del images, labels, outputs\n",
    "            train_accuracy = correct/total\n",
    "            accuracy_training_epochs.append(train_accuracy)\n",
    "\n",
    "        # Medición de la exactitud en el conjunto de prueba\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                del images, labels, outputs\n",
    "            t_acc = correct/total\n",
    "            test_accuracy.append(t_acc)\n",
    "            \n",
    "            # Guardar el modelo en caso de que su accuracy en el conjunto de prueba sea mayor que los anteriores\n",
    "            if t_acc >= max(test_accuracy):\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "\n",
    "        # Imprimir la pérdida, la exactitud en la validación y la exactitud en los datos de entrenamiento, de esta época.\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training accuracy: {round(train_accuracy,3)}, Validation accuracy: {round(val_accuracy,3)}, loss = {round(loss_epoch[-1],3)}\")\n",
    "        print(f\"Time spent on epoch {epoch+1}: {round((time.time()-start_time)/60,2)}min\")\n",
    "        \n",
    "    print(\"Entrenamiento finalizado\")\n",
    "        \n",
    "   # regresar el mejor modelo, el accuracy en el entrenamiento, validation y prueba y la pérdida. Info de todas las épocas.     \n",
    "    return [best_model,\n",
    "            accuracy_training_epochs,\n",
    "            accuracy_validation_epochs,\n",
    "            test_accuracy,\n",
    "            loss_epoch]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a52aab3",
   "metadata": {},
   "source": [
    "\n",
    "## Modelo base: ResNet-50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a03c5c",
   "metadata": {},
   "source": [
    "El modelo base es el ResNet-50, el cual fue introducido en el artículo [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385).\n",
    "\n",
    "Debido a las diferencias en tamaño entre CIFAR-10 y el conjunto de datos para el que fue diseñado el modelo ResNet-50 hemos hecho algunas modificaciones a la arquitectura original.\n",
    "\n",
    "Los detalles de cómo funciona el modelo ResNet-50, los cambios que introdujimos y la explicación del código pueden ser consultados en el notebook de \"./ResNet-50.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5163d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "class utilConv(nn.Sequential):\n",
    "    # groups=1 es la opción por defecto de una capa convolucional en pytorch, la defino para cambiarla más adelante en el notebook.\n",
    "    def __init__(self, in_features, out_features, kernel_size, stride = 1, norm = nn.BatchNorm2d, act = nn.ReLU, bias=True, groups=1):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_features, out_features, kernel_size=kernel_size ,padding=kernel_size // 2, stride=stride, bias=bias,groups=groups),\n",
    "            norm(out_features),\n",
    "            act()\n",
    "        )\n",
    "        \n",
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(self,in_features, out_features, reduction = 4, stride = 1):\n",
    "        super().__init__()\n",
    "        reduced_features = out_features // reduction\n",
    "        self.block = nn.Sequential(\n",
    "            # Reducción de canales\n",
    "            utilConv(in_features, reduced_features, kernel_size=1, stride=stride, bias=False), # el stride puede ser 2 para aplicar downsampling\n",
    "            # El número de canales se mantiene fijo\n",
    "            utilConv(reduced_features, reduced_features, kernel_size=3, bias=False),\n",
    "            # Aumento de canales\n",
    "            utilConv(reduced_features, out_features, kernel_size=1, bias=False, act=nn.Identity), \n",
    "        )\n",
    "        \n",
    "        # self.shortcut es utilizado para transformar al input a las dimensiones correctas para poder sumarlo a la salida del bloque\n",
    "        if in_features != out_features:\n",
    "            self.shortcut =nn.Sequential(utilConv(in_features, out_features, kernel_size=1, stride=stride, bias=False))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        res = self.shortcut(res)\n",
    "        x += res\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "    \n",
    "class Stage(nn.Sequential):\n",
    "    def __init__(self, in_features, out_features, depth, stride = 2):  # in_features y out_features deben ser distintos, sino se aplicará downsampling y el Bottleneck no aplicará la identidad\n",
    "        super().__init__(\n",
    "            \n",
    "            BottleNeckBlock(in_features, out_features, stride=stride), # Aquí se lleva a cabo el downsampling\n",
    "            *[BottleNeckBlock(out_features, out_features) for _ in range(depth - 1)]\n",
    "        )\n",
    "        \n",
    "        \n",
    "class Stem(nn.Sequential):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__(\n",
    "            utilConv(in_features, out_features, kernel_size=3, stride=1),  # en el caso de ImageNet, el kernel es de tamaño 7\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  \n",
    "        )\n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, stem_features, depths, widths):  # \n",
    "        super().__init__()\n",
    "        self.stem = Stem(in_channels, stem_features)\n",
    "\n",
    "        in_out_widths = list(zip(widths, widths[1:]))\n",
    "\n",
    "        \n",
    "        self.stages = nn.ModuleList() # lista de pytorch con los stages\n",
    "        \n",
    "        self.stages.append(Stage(stem_features, widths[0], depths[0], stride=1)) # se puede inferir de la figura 1 del artículo que el primer bloque del stage1 tiene stride 1\n",
    "        \n",
    "        for (in_features, out_features), depth in zip(in_out_widths, depths[1:]):\n",
    "            # añadir cada uno de los stages\n",
    "            self.stages.append(Stage(in_features, out_features, depth))\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        for stage in self.stages:\n",
    "\n",
    "            x = stage(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.decoder = nn.Linear(in_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.decoder(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e62321",
   "metadata": {},
   "source": [
    "Con estos bloques podemos definir nuestro modelo base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c0a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, n_classes, stem_features, depths, widths):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_channels=in_channels, stem_features=stem_features, depths=depths, widths=widths)\n",
    "        self.decoder = Decoder(widths[-1], n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d40510e",
   "metadata": {},
   "source": [
    "#### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ba9b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Repetimos 3 veces el experimento \n",
    "\n",
    "model1 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[3,4,6,3], widths=[64, 128, 256,512]).to(device)\n",
    "model2 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[3,4,6,3], widths=[64, 128, 256,512]).to(device)\n",
    "model3 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[3,4,6,3], widths=[64, 128, 256,512]).to(device)\n",
    "\n",
    "model1, training1, validation1, test1, loss1 = entrenamiento(model1, 2)\n",
    "model2, training2, validation2, test2, loss2 = entrenamiento(model2, 100)\n",
    "model3, training3, validation3, test3, loss3 = entrenamiento(model3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fba331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados\n",
    "results_dict1 = {\"loss\": loss1,\n",
    "    'Train':training1,\n",
    "     'Validation': validation1,\n",
    "     \"Test\":test1}\n",
    "results_dict2 = {\"loss\": loss2,\n",
    "    'Train':training2,\n",
    "     'Validation': validation2,\n",
    "     \"Test\":test2}\n",
    "results_dict3 = {\"loss\": loss3,\n",
    "    'Train':training3,\n",
    "     'Validation': validation3,\n",
    "     \"Test\":test3}\n",
    "\n",
    "results1_base = pd.DataFrame(results_dict1)\n",
    "results2_base = pd.DataFrame(results_dict2)\n",
    "results3_base = pd.DataFrame(results_dict3)\n",
    "\n",
    "results1_base.to_csv(\"./results/results_convnext_base_1.csv\",index=False)\n",
    "results2_base.to_csv(\"./results/results_convnext_base_2.csv\",index=False)\n",
    "results3_base.to_csv(\"./results/results_convnext_base_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_base = (results1_base[\"Test\"].max() + results2_base[\"Test\"].max() + results3_base[\"Test\"].max())/3\n",
    "print(f\"Accuracy del modelo base: {accuracy_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97481aed",
   "metadata": {},
   "source": [
    "## Cambiar la proporción de los stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3313384e",
   "metadata": {},
   "source": [
    "El primer cambio consiste en modificar el número de bloques *Bottleneck* que tiene cada *Stage*, originalmente se propone la proporción [3,3,9,3] en vez de [3,4,6,3] como en ResNet-50, sin embargo, para evitar aumentar el número de capas y aun así conservar la proporción [n,n,3n,n], hemos optado por [2,2,6,2].\n",
    "\n",
    "Para implementarlo solo hay que modificar el parámtro *depths* al crear el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389fe0c7",
   "metadata": {},
   "source": [
    "#### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a413eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repetimos 3 veces el experimento\n",
    "model1 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model2 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model3 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "\n",
    "model1, training1, validation1, test1, loss1 = entrenamiento(model1, 100)\n",
    "model2, training2, validation2, test2, loss2 = entrenamiento(model2, 100)\n",
    "model3, training3, validation3, test3, loss3 = entrenamiento(model3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa5eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados\n",
    "results_dict1 = {\"loss\": loss1,\n",
    "    'Train':training1,\n",
    "     'Validation': validation1,\n",
    "     \"Test\":test1}\n",
    "results_dict2 = {\"loss\": loss2,\n",
    "    'Train':training2,\n",
    "     'Validation': validation2,\n",
    "     \"Test\":test2}\n",
    "results_dict3 = {\"loss\": loss3,\n",
    "    'Train':training3,\n",
    "     'Validation': validation3,\n",
    "     \"Test\":test3}\n",
    "\n",
    "results1_change_stage_cr = pd.DataFrame(results_dict1)\n",
    "results2_change_stage_cr = pd.DataFrame(results_dict2)\n",
    "results3_change_stage_cr = pd.DataFrame(results_dict3)\n",
    "\n",
    "results1_change_stage_cr.to_csv(\"./results/results_convnext_change_stage_cr_1.csv\",index=False)\n",
    "results2_change_stage_cr.to_csv(\"./results/results_convnext_change_stage_cr_2.csv\",index=False)\n",
    "results3_change_stage_cr.to_csv(\"./results/results_convnext_change_stage_cr_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_cr = (results1_change_stage_cr[\"Test\"].max() + results2_change_stage_cr[\"Test\"].max() + results3_change_stage_cr[\"Test\"].max())/3\n",
    "print(f\"Accuracy del modelo al cambiar la proporción de los stages: {accuracy_cr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3f1ce",
   "metadata": {},
   "source": [
    "## Cambiar Stem por Patchify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d11fd",
   "metadata": {},
   "source": [
    "Las redes ResNet empiezan con un bloque *Stem*, éste se encarga de aplicar un *downsampling* agresivo a las imágenes de entrada. *Patchify*, por otro lado, consiste en dividir en bloques a las imágenes de entrada, mediante capas convolucionales cuyo stride es igual al tamaño del kernel.\n",
    "\n",
    "En el modelo particular que hemos estado usando optamos por evitar usar *downsampling* en el bloque *Stem* debido a que la dimensión de nuestras imágenes es más pequeña que la de las imágenes de ImageNet para las que fue planteado este diseño. Por ello mismo, hemos optado por utilizar *patchify* de dimensión 1, para evitar reducir el tamaño de las imágenes. Originalmente se utiliza *patchify* de dimensión 4.\n",
    "\n",
    "Para implementar esto, redefinimos el bloque Stem para que consista de una capa convolucional seguida de una capa de Batch Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f30bafbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stem(nn.Sequential):\n",
    "    def __init__(self, in_features, out_features, patch_size=1):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_features, out_features, kernel_size=patch_size, stride=patch_size),\n",
    "            nn.BatchNorm2d(out_features) \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178218a6",
   "metadata": {},
   "source": [
    "#### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d233d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repetimos 3 veces el experimento\n",
    "model1 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model2 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model3 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "\n",
    "model1, training1, validation1, test1, loss1 = entrenamiento(model1, 100)\n",
    "model2, training2, validation2, test2, loss2 = entrenamiento(model2, 100)\n",
    "model3, training3, validation3, test3, loss3 = entrenamiento(model3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8016f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados\n",
    "results_dict1 = {\"loss\": loss1,\n",
    "    'Train':training1,\n",
    "     'Validation': validation1,\n",
    "     \"Test\":test1}\n",
    "results_dict2 = {\"loss\": loss2,\n",
    "    'Train':training2,\n",
    "     'Validation': validation2,\n",
    "     \"Test\":test2}\n",
    "results_dict3 = {\"loss\": loss3,\n",
    "    'Train':training3,\n",
    "     'Validation': validation3,\n",
    "     \"Test\":test3}\n",
    "\n",
    "results1_patchify = pd.DataFrame(results_dict1)\n",
    "results2_patchify = pd.DataFrame(results_dict2)\n",
    "results3_patchify = pd.DataFrame(results_dict3)\n",
    "\n",
    "results1_patchify.to_csv(\"./results/results_convnext_patchify_1.csv\",index=False)\n",
    "results2_patchify.to_csv(\"./results/results_convnext_patchify_2.csv\",index=False)\n",
    "results3_patchify.to_csv(\"./results/results_convnext_patchify_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfe85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_patch = (results1_patchify[\"Test\"].max() + results2_patchify[\"Test\"].max() + results3_patchify[\"Test\"].max())/3\n",
    "print(f\"Accuracy del modelo al aplicar Patchify: {accuracy_patch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ae242",
   "metadata": {},
   "source": [
    "## Añadir *depthwise convolution* (*ResNeXt-ify*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb149bc8",
   "metadata": {},
   "source": [
    "Basados en el artículo [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/abs/1611.05431), se propone utilizar *grouped convolution* en la capa convolucional en la que el kernel es de tamaño 3 del bloque Bottleneck . En particular, se utiliza un tipo de *grouped convolution* llamdo *depthwise*, en donde el número de grupos es el mismo que el número de canales de entrada.\n",
    "\n",
    "Para implementar este cambio basta utilizar el parámetro *groups* al llamar a la capa nn.Conv2d() en cuestión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(self,in_features, out_features, reduction = 4, stride = 1):\n",
    "        super().__init__()\n",
    "        reduced_features = out_features // reduction\n",
    "        self.block = nn.Sequential(\n",
    "            # Reducción de canales\n",
    "            utilConv(in_features, reduced_features, kernel_size=1, stride=stride, bias=False),\n",
    "            # El número de canales se mantiene fijo\n",
    "            utilConv(reduced_features, reduced_features, kernel_size=3, bias=False, groups=reduced_features), # en esta capa se utiliza grouped convolution\n",
    "            # Aumento de canales\n",
    "            utilConv(reduced_features, out_features, kernel_size=1, bias=False, act=nn.Identity), \n",
    "        )\n",
    "        \n",
    "        # self.shortcut es utilizado para transformar al input a las dimensiones correctas para poder sumarlo a la salida del bloque\n",
    "        if in_features != out_features:\n",
    "            self.shortcut =nn.Sequential(utilConv(in_features, out_features, kernel_size=1, stride=stride, bias=False))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        res = self.shortcut(res)\n",
    "        x += res\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d93bb",
   "metadata": {},
   "source": [
    "#### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cefe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repetimos 3 veces el experimento\n",
    "model1 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model2 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model3 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "\n",
    "model1, training1, validation1, test1, loss1 = entrenamiento(model1, 100)\n",
    "model2, training2, validation2, test2, loss2 = entrenamiento(model2, 100)\n",
    "model3, training3, validation3, test3, loss3 = entrenamiento(model3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c678af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados\n",
    "results_dict1 = {\"loss\": loss1,\n",
    "    'Train':training1,\n",
    "     'Validation': validation1,\n",
    "     \"Test\":test1}\n",
    "results_dict2 = {\"loss\": loss2,\n",
    "    'Train':training2,\n",
    "     'Validation': validation2,\n",
    "     \"Test\":test2}\n",
    "results_dict3 = {\"loss\": loss3,\n",
    "    'Train':training3,\n",
    "     'Validation': validation3,\n",
    "     \"Test\":test3}\n",
    "\n",
    "results1_resnextify = pd.DataFrame(results_dict1)\n",
    "results2_resnextify = pd.DataFrame(results_dict2)\n",
    "results3_resnextify = pd.DataFrame(results_dict3)\n",
    "\n",
    "results1_resnextify.to_csv(\"./results/results_convnext_resnextify_1.csv\",index=False)\n",
    "results2_resnextify.to_csv(\"./results/results_convnext_resnextify_2.csv\",index=False)\n",
    "results3_resnextify.to_csv(\"./results/results_convnext_resnextify_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed95c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_resnext = (results1_resnextify[\"Test\"].max() + results2_resnextify[\"Test\"].max() + results3_resnextify[\"Test\"].max())/3\n",
    "print(f\"Accuracy del modelo al aplicar  ResNeXt-ify: {accuracy_resnext}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623093cf",
   "metadata": {},
   "source": [
    "## Inverted-Bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a74ddc",
   "metadata": {},
   "source": [
    "El modelo ResNet-50 utiliza el bloque Bottleneck. El nombre de dicho bloque se debe a que primero reduce el número de canales mediante una convolución con kernel de tamaño 1, después mantiene el número de canales y aplica una convolución con kernel de tamaño 3 y por último aumenta el número de canales al original.\n",
    "\n",
    "Los transformadores de visión utilizan un sistema opuesto. Primero aumentan el número de canales, después aplican la convolución con kernel de dimensión mayor a 1 y por último reducen el número de canales al original. Este tipo de bloque se llama *Inverted-Bottleneck*, o Bottleneck invertido.\n",
    "\n",
    "Inspirados en el diseño de los transformadores de visión, se propone utilizar bloques Bottleneck invertidos. La implementación se puede llevar a cabo mediante la modificación del bloque BottleNeck como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13447ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, expansion = 4, stride = 1):\n",
    "        super().__init__()\n",
    "        expanded_features = out_features * expansion\n",
    "        self.block = nn.Sequential(\n",
    "            # Aumento de canales\n",
    "            utilConv(in_features, expanded_features, kernel_size=1, stride=stride, bias=False),\n",
    "            # El número de canales se mantiene fijo (Aquí se aplica la convolución depthwise)\n",
    "            utilConv(expanded_features, expanded_features, kernel_size=3, bias=False, groups=in_features),\n",
    "            # Reducción de canales\n",
    "            utilConv(expanded_features, out_features, kernel_size=1, bias=False, act=nn.Identity)\n",
    "        )\n",
    "\n",
    "        # self.shortcut es utilizado para transformar al input a las dimensiones correctas para poder sumarlo a la salida del bloque\n",
    "        if in_features != out_features:\n",
    "            self.shortcut =nn.Sequential(utilConv(in_features, out_features, kernel_size=1, stride=stride, bias=False))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        res = self.shortcut(res)\n",
    "        x += res\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adbfc26",
   "metadata": {},
   "source": [
    "#### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0329ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repetimos 3 veces el experimento\n",
    "model1 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model2 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model3 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "\n",
    "model1, training1, validation1, test1, loss1 = entrenamiento(model1, 100)\n",
    "model2, training2, validation2, test2, loss2 = entrenamiento(model2, 100)\n",
    "model3, training3, validation3, test3, loss3 = entrenamiento(model3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7916482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados\n",
    "results_dict1 = {\"loss\": loss1,\n",
    "    'Train':training1,\n",
    "     'Validation': validation1,\n",
    "     \"Test\":test1}\n",
    "results_dict2 = {\"loss\": loss2,\n",
    "    'Train':training2,\n",
    "     'Validation': validation2,\n",
    "     \"Test\":test2}\n",
    "results_dict3 = {\"loss\": loss3,\n",
    "    'Train':training3,\n",
    "     'Validation': validation3,\n",
    "     \"Test\":test3}\n",
    "\n",
    "results1_inverted = pd.DataFrame(results_dict1)\n",
    "results2_inverted = pd.DataFrame(results_dict2)\n",
    "results3_inverted = pd.DataFrame(results_dict3)\n",
    "\n",
    "results1_inverted.to_csv(\"./results/results_convnext_inverted_1.csv\",index=False)\n",
    "results2_inverted.to_csv(\"./results/results_convnext_inverted_2.csv\",index=False)\n",
    "results3_inverted.to_csv(\"./results/results_convnext_inverted_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_inverted = (results1_inverted[\"Test\"].max() + results2_inverted[\"Test\"].max() + results3_inverted[\"Test\"].max())/3\n",
    "print(f\"Accuracy del modelo al invertir el cuello de botella: {accuracy_inverted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120245bd",
   "metadata": {},
   "source": [
    "## Aumentar el tamaño de los kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0424ddd",
   "metadata": {},
   "source": [
    "Tomando como inspiración el modelo [Swin Transformers](https://arxiv.org/abs/2103.14030), se propone aumentar el tamaño del kernel de las capas convolucionales en el bloque Bottleneck, de 3x3 a 7x7. Para ello se plantea que es necesario hacer dos cosas:\n",
    "\n",
    "1. Mover la capa de *depthwise convolution* para que sea la primera del bloque\n",
    "2. Incrementar el tamaño del kernel de la capa de *depthwise convolution* a 7x7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3de65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, expansion = 4, stride = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        expanded_features = out_features * expansion\n",
    "        self.block = nn.Sequential(\n",
    "            # El número de canales se mantiene fijo (con grupos depth-wise y kernel más grande)\n",
    "            utilConv(in_features, in_features, kernel_size=7, stride=stride, bias=False, groups=in_features),\n",
    "            # Aumento en el número de canales\n",
    "            utilConv(in_features, expanded_features, kernel_size=1),\n",
    "            # Reducción de canales\n",
    "            utilConv(expanded_features, out_features, kernel_size=1, bias=False, act=nn.Identity),\n",
    "        )\n",
    "        \n",
    "        # self.shortcut es utilizado para transformar al input a las dimensiones correctas para poder sumarlo a la salida del bloque\n",
    "        if in_features != out_features:\n",
    "            self.shortcut =nn.Sequential(utilConv(in_features, out_features, kernel_size=1, stride=stride, bias=False))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        res = self.shortcut(res)\n",
    "        x += res\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23703e78",
   "metadata": {},
   "source": [
    "#### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repetimos 3 veces el experimento\n",
    "model1 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model2 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model3 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "\n",
    "model1, training1, validation1, test1, loss1 = entrenamiento(model1, 100)\n",
    "model2, training2, validation2, test2, loss2 = entrenamiento(model2, 100)\n",
    "model3, training3, validation3, test3, loss3 = entrenamiento(model3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e607d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados\n",
    "results_dict1 = {\"loss\": loss1,\n",
    "    'Train':training1,\n",
    "     'Validation': validation1,\n",
    "     \"Test\":test1}\n",
    "results_dict2 = {\"loss\": loss2,\n",
    "    'Train':training2,\n",
    "     'Validation': validation2,\n",
    "     \"Test\":test2}\n",
    "results_dict3 = {\"loss\": loss3,\n",
    "    'Train':training3,\n",
    "     'Validation': validation3,\n",
    "     \"Test\":test3}\n",
    "\n",
    "results1_kernel = pd.DataFrame(results_dict1)\n",
    "results2_kernel = pd.DataFrame(results_dict2)\n",
    "results3_kernel = pd.DataFrame(results_dict3)\n",
    "\n",
    "results1_kernel.to_csv(\"./results/results_convnext_kernel_1.csv\",index=False)\n",
    "results2_kernel.to_csv(\"./results/results_convnext_kernel_2.csv\",index=False)\n",
    "results3_kernel.to_csv(\"./results/results_convnext_kernel_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1269ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_kernel = (results1_kernel[\"Test\"].max() + results2_kernel[\"Test\"].max() + results3_kernel[\"Test\"].max())/3\n",
    "print(f\"Accuracy del modelo al aumentar el tamaño de los kernels: {accuracy_kernel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7aa59",
   "metadata": {},
   "source": [
    "## Cambios en el micro-diseño"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8796a8",
   "metadata": {},
   "source": [
    "Por último se proponen los siguientes cambios de menor escala en el diseño de la arquitectura:\n",
    "\n",
    "1. Cambiar la función de activación ReLU por GELU\n",
    "2. Disminuir el número de funciones de activación\n",
    "3. Disminución en el número de capas de normalización\n",
    "4. Cambiar *batch normalization* por *layer normalization*\n",
    "5. Separar las capas de *downsampling*\n",
    "6. Añadir *Stochastic Depth*, también conocido como *Drop Path* y *Layer Scale*\n",
    "\n",
    "Nuevamente, debido a que nuestras imágenes son de dimensión 32x32, en vez de 224x224 como fue propuesto en el artículo, optamos por omitir la primera capa de downsampling.\n",
    "\n",
    "Implementemos dichos cambios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "305fe736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerScaler(nn.Module):\n",
    "    def __init__(self, init_value, dimensions):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(init_value * torch.ones((dimensions)), \n",
    "                                    requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.gamma[None,...,None,None] * x\n",
    "\n",
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, expansion = 4, drop_p = .0, layer_scaler_init_value = 1e-6):\n",
    "        super().__init__()\n",
    "        expanded_features = out_features * expansion\n",
    "        self.block = nn.Sequential(\n",
    "            # El número de canales se mantiene fijo (con grupos depth-wise y kernel más grande)\n",
    "            nn.Conv2d(\n",
    "                in_features, in_features, kernel_size=7, padding=3, bias=False, groups=in_features\n",
    "            ),\n",
    "            # nn.GroupNorm(num_groups=1) nos permite aplicar LayerNorm\n",
    "            nn.GroupNorm(num_groups=1, num_channels=in_features),\n",
    "            # Aumento en el número de canales \n",
    "            nn.Conv2d(in_features, expanded_features, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            # Reducción de canales\n",
    "            nn.Conv2d(expanded_features, out_features, kernel_size=1),\n",
    "        )\n",
    "        self.layer_scaler = LayerScaler(layer_scaler_init_value, out_features)\n",
    "        self.drop_path = StochasticDepth(drop_p, mode=\"batch\")\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        x = self.layer_scaler(x)\n",
    "        x = self.drop_path(x)\n",
    "        x += res\n",
    "        return x\n",
    "    \n",
    "class Stage(nn.Sequential):\n",
    "    def __init__(self, in_features, out_features, depth, drop_p = .0):\n",
    "        \n",
    "        if in_features != out_features:\n",
    "            super().__init__(\n",
    "                # Añadimos la capa de downsampling previo al stage.\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(num_groups=1, num_channels=in_features),\n",
    "                    nn.Conv2d(in_features, out_features, kernel_size=2, stride=2)\n",
    "                ),\n",
    "                # Añadimos los stages\n",
    "                *[BottleNeckBlock(out_features, out_features, drop_p=drop_p) for _ in range(depth)]\n",
    "            )\n",
    "        else:\n",
    "            super().__init__(\n",
    "                # Con esto garantizamos que no haya downsampling previo al primer stage\n",
    "                *[BottleNeckBlock(out_features, out_features, drop_p=drop_p) for _ in range(depth)]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a176af",
   "metadata": {},
   "source": [
    "#### Construimos el enconder y decoder de ConvNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24b7af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, stem_features, depths, widths, drop_p = .0):\n",
    "        super().__init__()\n",
    "        self.stem = Stem(in_channels, stem_features)\n",
    "\n",
    "        in_out_widths = list(zip(widths, widths[1:]))\n",
    "        # Probabilidades para la implementación de Drop Path \n",
    "        drop_probs = [x.item() for x in torch.linspace(0, drop_p, sum(depths))]     \n",
    "        \n",
    "        self.stages = nn.ModuleList()\n",
    "        self.stages.append(Stage(stem_features, widths[0], depths[0], drop_p=drop_probs[0]))\n",
    "        for (in_features, out_features), depth, drop_p in zip(in_out_widths, depths[1:], drop_probs[1:]):\n",
    "            self.stages.append(Stage(in_features, out_features, depth, drop_p=drop_p))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49c1591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextDecoder(nn.Sequential):\n",
    "    def __init__(self, num_channels, num_classes = 10):\n",
    "        super().__init__(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(1),\n",
    "            nn.LayerNorm(num_channels),\n",
    "            nn.Linear(num_channels, num_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88580c8",
   "metadata": {},
   "source": [
    "#### Definición del modelo final Modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "438ef397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNext(nn.Sequential):\n",
    "    def __init__(self, in_channels, stem_features, depths, widths, drop_p = .0, num_classes = 10):\n",
    "        super().__init__()\n",
    "        self.encoder = ConvNextEncoder(in_channels, stem_features, depths, widths, drop_p)\n",
    "        self.head = ConvNextDecoder(widths[-1], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cec02557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 8,531,978\n"
     ]
    }
   ],
   "source": [
    "# Número de parámetros del modelo\n",
    "model = ConvNext(3,64,[2,2,6,2],[64, 128, 256, 512])\n",
    "print(\"Number of parameters: {:,}\".format(sum(p.numel() for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e87d14a",
   "metadata": {},
   "source": [
    "#### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea2c32e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando entrenamiento\n",
      "Epoch [1/100], Training accuracy: 0.403, Validation accuracy: 0.407, loss = 1.489\n",
      "Time spent on epoch 1: 6.21min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model2 \u001b[38;5;241m=\u001b[39m ConvNext(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m64\u001b[39m,[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m2\u001b[39m],[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m512\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m model3 \u001b[38;5;241m=\u001b[39m ConvNext(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m64\u001b[39m,[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m2\u001b[39m],[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m512\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 6\u001b[0m model1, training1, validation1, test1, loss1 \u001b[38;5;241m=\u001b[39m \u001b[43mentrenamiento\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model2, training2, validation2, test2, loss2 \u001b[38;5;241m=\u001b[39m entrenamiento(model2, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      8\u001b[0m model3, training3, validation3, test3, loss3 \u001b[38;5;241m=\u001b[39m entrenamiento(model3, \u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 48\u001b[0m, in \u001b[0;36mentrenamiento\u001b[1;34m(model, epocas)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m images, labels, outputs\n\u001b[0;32m     47\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m---> 48\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m     50\u001b[0m loss_epoch\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;66;03m# Guardar la información del loss de esta época\u001b[39;00m\n\u001b[0;32m     51\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Implementación de learning rate decay\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Repetimos 3 veces el experimento\n",
    "model1 = ConvNext(3,64,[2,2,6,2],[64, 128, 256, 512]).to(device)\n",
    "model2 = ConvNext(3,64,[2,2,6,2],[64, 128, 256, 512]).to(device)\n",
    "model3 = ConvNext(3,64,[2,2,6,2],[64, 128, 256, 512]).to(device)\n",
    "\n",
    "model1, training1, validation1, test1, loss1 = entrenamiento(model1, 100)\n",
    "model2, training2, validation2, test2, loss2 = entrenamiento(model2, 100)\n",
    "model3, training3, validation3, test3, loss3 = entrenamiento(model3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5b01a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados\n",
    "results_dict1 = {\"loss\": loss1,\n",
    "    'Train':training1,\n",
    "     'Validation': validation1,\n",
    "     \"Test\":test1}\n",
    "results_dict2 = {\"loss\": loss2,\n",
    "    'Train':training2,\n",
    "     'Validation': validation2,\n",
    "     \"Test\":test2}\n",
    "results_dict3 = {\"loss\": loss3,\n",
    "    'Train':training3,\n",
    "     'Validation': validation3,\n",
    "     \"Test\":test3}\n",
    "\n",
    "results1_final = pd.DataFrame(results_dict1)\n",
    "results2_final = pd.DataFrame(results_dict2)\n",
    "results3_final = pd.DataFrame(results_dict3)\n",
    "\n",
    "results1_final.to_csv(\"./results/results_convnext_final_1.csv\",index=False)\n",
    "results2_final.to_csv(\"./results/results_convnext_final_2.csv\",index=False)\n",
    "results3_final.to_csv(\"./results/results_convnext_final_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_final = (results1_final[\"Test\"].max() + results2_final[\"Test\"].max() + results3_final[\"Test\"].max())/3\n",
    "print(f\"Accuracy del modelo ConvNeXt: {accuracy_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04adecdf",
   "metadata": {},
   "source": [
    "## Resultados y Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb713cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
