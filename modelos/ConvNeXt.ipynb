{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8846f0e1",
   "metadata": {},
   "source": [
    "# Modelo ConvNeXt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b77232",
   "metadata": {},
   "source": [
    "En este notebook explicaremos el proceso para construir una red neuronal convolucional *ConvNeXt* a partir de una red neuronal *ResNet*. Nos basaremos en el artículo [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545), donde fueron introducidas las redes *ConvNeXt*.\n",
    "\n",
    "El proceso consiste en siete pasos:\n",
    "\n",
    "1. **Crear un modelo base de ResNet basado en ResNet-50**\n",
    "2. **Cambiar la proporción de los stages del modelo ResNet**\n",
    "3. **Cambiar el bloque *Stem* por un bloque *Patchify***\n",
    "4. **Añadir *depthwise convolution* (*ResNeXt-ify*)**\n",
    "5. **Cambiar el bloque *Bottleneck* por *Inverted-Bottleneck***\n",
    "6. **Aumentar el tamaño de los kernels**\n",
    "7. **Cambios en el *microdiseño***\n",
    "\n",
    "\n",
    "Para medir los cambios en la exactitud (*accuracy*) del modelo en cada uno de los siete pasos, entrenaremos al correspondiente modelo por 100 épocas utilizando los mismos parámetros de entrenamiento. Repetiremos tres veces cada experimento y reportaremos el promedio de los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d3cc99",
   "metadata": {},
   "source": [
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2367153",
   "metadata": {},
   "source": [
    "Entrenaremos al modelo en el conjunto de datos CIFAR-10. Este dataset consiste de 60000 imágenes a color en 10 clases distintas, donde no hay intersección entre las distintas clases. Se puede acceder al dataset mediante las herramientas de la paquetería de pytorch, o también en la página oficial: https://www.cs.toronto.edu/~kriz/cifar.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dc37dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las paqueterías necesarias para el notebook\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import StochasticDepth\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# De ser posible utilizaremos GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96d53aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def data_loader(data_dir,\n",
    "                batch_size,\n",
    "                random_seed=42,\n",
    "                valid_size=0.1,\n",
    "                shuffle=False,\n",
    "                test=False):\n",
    "    \"\"\"\n",
    "    Función para cargar los datos de CIFAR-10\n",
    "    \"\"\"\n",
    "    \n",
    "    # Definimos el transform para normalizar los datos con pytorch\n",
    "    # Los valores fueron obtenidos en el notebook \"data_extraction.ipynb\"\n",
    "    normalize = transforms.Normalize(  \n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # Definimos el transform para preporcesar los datos\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    # Obtener los datos del conjunto de prueba\n",
    "    if test:\n",
    "        dataset = datasets.CIFAR10(\n",
    "          root=data_dir, train=False,\n",
    "          download=True, transform=transform_test,\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    # Cargamos una copia de los datos de entrenamiento\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform_train,\n",
    "    )\n",
    "    \n",
    "    # Cargamos una copia extra de los datos de entrenamiento para dividirlo después en el conjunto de validación\n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform_train,\n",
    "    )\n",
    "    \n",
    "    # Separamos los datos de entrenamiento y validación mediante índices\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    # Finalmente, definimos los conjuntos de entrenamiento y validación\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "\n",
    "# Aplicamos la función para cargar los datos de CIFAR-10, los guardamos en el directorio actual\n",
    "train_loader, valid_loader = data_loader(data_dir='./data',\n",
    "                                         batch_size=64)\n",
    "\n",
    "test_loader = data_loader(data_dir='./data',\n",
    "                              batch_size=64,\n",
    "                              test=True)    \n",
    "cifar10_classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f1a31",
   "metadata": {},
   "source": [
    "### Función de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ab333",
   "metadata": {},
   "source": [
    "Con la siguiente función obtendremos los datos del desempeño de cada modelo a lo largo del entrenamiento. Los parámetros de entrenamiento son los mismo que los utilizados en el artículo donde fueorn introducidas las redes [*ConvNeXt*](https://arxiv.org/abs/2201.03545), con la excepción del número de épocas, el cual no demostró mejorar el desempeño de la red al ser aumentado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4777d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenamiento(model, epocas):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # variables para guardar los resultados\n",
    "    accuracy_training_epochs = []\n",
    "    accuracy_validation_epochs = []\n",
    "    loss_epoch = []\n",
    "    test_accuracy = []\n",
    "    best_model = None\n",
    "    \n",
    "    # parámetros de entrenamiento\n",
    "    num_epochs = epocas\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(),\n",
    "                            lr=0.004,\n",
    "                            betas=(0.9, 0.999),\n",
    "                            weight_decay=0.05\n",
    "                            )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epocas)\n",
    "\n",
    "    \n",
    "    # entrenamiento\n",
    "    print(\"Comenzando entrenamiento\")\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "            # Mover a los tensores a GPU de ser posible\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Ahorro de memoria\n",
    "            del images, labels, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        loss_epoch.append(loss.item()) # Guardar la información del loss de esta época\n",
    "        lr_scheduler.step() # Implementación de learning rate decay\n",
    "\n",
    "        # Medición de la exactitud en el conjunto de validación\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in valid_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                del images, labels, outputs\n",
    "            val_accuracy = correct/total\n",
    "            accuracy_validation_epochs.append(val_accuracy)\n",
    "\n",
    "        # Medición de la exactitud sobre todo el conjunto de entrenamiento\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in train_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                del images, labels, outputs\n",
    "            train_accuracy = correct/total\n",
    "            accuracy_training_epochs.append(train_accuracy)\n",
    "\n",
    "        # Medición de la exactitud en el conjunto de prueba\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                del images, labels, outputs\n",
    "            t_acc = correct/total\n",
    "            test_accuracy.append(t_acc)\n",
    "            \n",
    "            # Guardar el modelo en caso de que su accuracy en el conjunto de prueba sea mayor que los anteriores\n",
    "            if t_acc >= max(test_accuracy):\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "\n",
    "        # Imprimir la pérdida, la exactitud en la validación y la exactitud en los datos de entrenamiento, de esta época.\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training accuracy: {round(train_accuracy,3)}, Validation accuracy: {round(val_accuracy,3)}, loss = {round(loss_epoch[-1],3)}\")\n",
    "        print(f\"Time spent on epoch {epoch+1}: {round((time.time()-start_time)/60,2)}min\")\n",
    "        \n",
    "    print(\"Entrenamiento finalizado\")\n",
    "        \n",
    "   # regresar el mejor modelo, el accuracy en el entrenamiento, validation y prueba y la pérdida. Info de todas las épocas.     \n",
    "    return [best_model,\n",
    "            accuracy_training_epochs,\n",
    "            accuracy_validation_epochs,\n",
    "            test_accuracy,\n",
    "            loss_epoch]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a52aab3",
   "metadata": {},
   "source": [
    "\n",
    "## Modelo base: ResNet-50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a03c5c",
   "metadata": {},
   "source": [
    "El modelo base es el ResNet-50, el cual fue introducido en el artículo [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385).\n",
    "\n",
    "Debido a las diferencias en tamaño entre CIFAR-10 y el conjunto de datos para el que fue diseñado el modelo ResNet-50 hemos hecho algunas modificaciones a la arquitectura original.\n",
    "\n",
    "Los detalles de cómo funciona el modelo ResNet-50, los cambios que introdujimos y la explicación del código pueden ser consultados en el notebook de \"./ResNet-50.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5163d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "class utilConv(nn.Sequential):\n",
    "    # groups=1 es la opción por defecto de una capa convolucional en pytorch, la defino para cambiarla más adelante en el notebook.\n",
    "    def __init__(self, in_features, out_features, kernel_size, stride = 1, norm = nn.BatchNorm2d, act = nn.ReLU, bias=True, groups=1):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_features, out_features, kernel_size=kernel_size ,padding=kernel_size // 2, stride=stride, bias=bias,groups=groups),\n",
    "            norm(out_features),\n",
    "            act()\n",
    "        )\n",
    "        \n",
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(self,in_features, out_features, reduction = 4, stride = 1):\n",
    "        super().__init__()\n",
    "        reduced_features = out_features // reduction\n",
    "        self.block = nn.Sequential(\n",
    "            # Reducción de canales\n",
    "            utilConv(in_features, reduced_features, kernel_size=1, stride=stride, bias=False), # el stride puede ser 2 para aplicar downsampling\n",
    "            # El número de canales se mantiene fijo\n",
    "            utilConv(reduced_features, reduced_features, kernel_size=3, bias=False),\n",
    "            # Aumento de canales\n",
    "            utilConv(reduced_features, out_features, kernel_size=1, bias=False, act=nn.Identity), \n",
    "        )\n",
    "        \n",
    "        # self.shortcut es utilizado para transformar al input a las dimensiones correctas para poder sumarlo a la salida del bloque\n",
    "        if in_features != out_features:\n",
    "            self.shortcut =nn.Sequential(utilConv(in_features, out_features, kernel_size=1, stride=stride, bias=False))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        res = self.shortcut(res)\n",
    "        x += res\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "    \n",
    "class Stage(nn.Sequential):\n",
    "    def __init__(self, in_features, out_features, depth, stride = 2):  # in_features y out_features deben ser distintos, sino se aplicará downsampling y el Bottleneck no aplicará la identidad\n",
    "        super().__init__(\n",
    "            \n",
    "            BottleNeckBlock(in_features, out_features, stride=stride), # Aquí se lleva a cabo el downsampling\n",
    "            *[BottleNeckBlock(out_features, out_features) for _ in range(depth - 1)]\n",
    "        )\n",
    "        \n",
    "        \n",
    "class Stem(nn.Sequential):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__(\n",
    "            utilConv(in_features, out_features, kernel_size=3, stride=1),  # en el caso de ImageNet, el kernel es de tamaño 7\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  \n",
    "        )\n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, stem_features, depths, widths):  # \n",
    "        super().__init__()\n",
    "        self.stem = Stem(in_channels, stem_features)\n",
    "\n",
    "        in_out_widths = list(zip(widths, widths[1:]))\n",
    "\n",
    "        \n",
    "        self.stages = nn.ModuleList() # lista de pytorch con los stages\n",
    "        \n",
    "        self.stages.append(Stage(stem_features, widths[0], depths[0], stride=1)) # se puede inferir de la figura 1 del artículo que el primer bloque del stage1 tiene stride 1\n",
    "        \n",
    "        for (in_features, out_features), depth in zip(in_out_widths, depths[1:]):\n",
    "            # añadir cada uno de los stages\n",
    "            self.stages.append(Stage(in_features, out_features, depth))\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        for stage in self.stages:\n",
    "\n",
    "            x = stage(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.decoder = nn.Linear(in_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.decoder(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e62321",
   "metadata": {},
   "source": [
    "Con estos bloques podemos definir nuestro modelo base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81c0a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, n_classes, stem_features, depths, widths):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_channels=in_channels, stem_features=stem_features, depths=depths, widths=widths)\n",
    "        self.decoder = Decoder(widths[-1], n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d40510e",
   "metadata": {},
   "source": [
    "#### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df7ba9b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Training accuracy: 0.299, Validation accuracy: 0.288, loss = 2.249\n",
      "Time spent on epoch 1: 3.36min\n",
      "Epoch [2/2], Training accuracy: 0.337, Validation accuracy: 0.335, loss = 1.93\n",
      "Time spent on epoch 2: 3.38min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m model3 \u001b[38;5;241m=\u001b[39m ResNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, stem_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, depths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m3\u001b[39m], widths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m512\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m model1, training1, validation1, test1, loss1 \u001b[38;5;241m=\u001b[39m entrenamiento(model1, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m model2, training2, validation2, test2, loss2 \u001b[38;5;241m=\u001b[39m \u001b[43mentrenamiento\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m model3, training3, validation3, test3, loss3 \u001b[38;5;241m=\u001b[39m entrenamiento(model3, \u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 47\u001b[0m, in \u001b[0;36mentrenamiento\u001b[1;34m(model, epocas)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m images, labels, outputs\n\u001b[0;32m     46\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m---> 47\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m     49\u001b[0m loss_epoch\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;66;03m# Guardar la información del loss de esta época\u001b[39;00m\n\u001b[0;32m     50\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Implementación de learning rate decay\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Repetimos 3 veces el experimento \n",
    "\n",
    "model1 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[3,4,6,3], widths=[64, 128, 256,512]).to(device)\n",
    "model2 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[3,4,6,3], widths=[64, 128, 256,512]).to(device)\n",
    "model3 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[3,4,6,3], widths=[64, 128, 256,512]).to(device)\n",
    "\n",
    "model1, training1, validation1, test1, loss1 = entrenamiento(model1, 2)\n",
    "model2, training2, validation2, test2, loss2 = entrenamiento(model2, 100)\n",
    "model3, training3, validation3, test3, loss3 = entrenamiento(model3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fba331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados\n",
    "results_dict1 = {\"loss\": loss1,\n",
    "    'Train':training1,\n",
    "     'Validation': validation1,\n",
    "     \"Test\":test1}\n",
    "results_dict2 = {\"loss\": loss2,\n",
    "    'Train':training2,\n",
    "     'Validation': validation2,\n",
    "     \"Test\":test2}\n",
    "results_dict3 = {\"loss\": loss3,\n",
    "    'Train':training3,\n",
    "     'Validation': validation3,\n",
    "     \"Test\":test3}\n",
    "\n",
    "results1_base = pd.DataFrame(results_dict1)\n",
    "results2_base = pd.DataFrame(results_dict2)\n",
    "results3_base = pd.DataFrame(results_dict3)\n",
    "\n",
    "results1_base.to_csv(\"./results/results_convnext_base_1.csv\",index=False)\n",
    "results2_base.to_csv(\"./results/results_convnext_base_2.csv\",index=False)\n",
    "results3_base.to_csv(\"./results/results_convnext_base_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_base = (results1_base[\"Test\"].max() + results2_base[\"Test\"].max() + results3_base[\"Test\"].max())/3\n",
    "print(f\"Accuracy del modelo base: {accuracy_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97481aed",
   "metadata": {},
   "source": [
    "## Cambiar la proporción de los stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3313384e",
   "metadata": {},
   "source": [
    "El primer cambio consiste en modificar el número de bloques *Bottleneck* que tiene cada *Stage*, originalmente se propone la proporción [3,3,9,3] en vez de [3,4,6,3] como en ResNet-50, sin embargo, para evitar aumentar el número de capas y aun así conservar la proporción [n,n,3n,n], hemos optado por [2,2,6,2].\n",
    "\n",
    "Para implementarlo solo hay que modificar el parámtro *depths* al crear el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389fe0c7",
   "metadata": {},
   "source": [
    "#### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a413eb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model2 \u001b[38;5;241m=\u001b[39m ResNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, stem_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, depths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m2\u001b[39m], widths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m512\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m model3 \u001b[38;5;241m=\u001b[39m ResNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, stem_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, depths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m2\u001b[39m], widths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m512\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 6\u001b[0m model1, training1, validation1, test1, loss1 \u001b[38;5;241m=\u001b[39m \u001b[43mentrenamiento\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model2, training2, validation2, test2, loss2 \u001b[38;5;241m=\u001b[39m entrenamiento(model2, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      8\u001b[0m model3, training3, validation3, test3, loss3 \u001b[38;5;241m=\u001b[39m entrenamiento(model3, \u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 41\u001b[0m, in \u001b[0;36mentrenamiento\u001b[1;34m(model, epocas)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 41\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Ahorro de memoria\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Repetimos 3 veces el experimento\n",
    "model1 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model2 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model3 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "\n",
    "model1, training1, validation1, test1, loss1 = entrenamiento(model1, 100)\n",
    "model2, training2, validation2, test2, loss2 = entrenamiento(model2, 100)\n",
    "model3, training3, validation3, test3, loss3 = entrenamiento(model3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa5eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados\n",
    "results_dict1 = {\"loss\": loss1,\n",
    "    'Train':training1,\n",
    "     'Validation': validation1,\n",
    "     \"Test\":test1}\n",
    "results_dict2 = {\"loss\": loss2,\n",
    "    'Train':training2,\n",
    "     'Validation': validation2,\n",
    "     \"Test\":test2}\n",
    "results_dict3 = {\"loss\": loss3,\n",
    "    'Train':training3,\n",
    "     'Validation': validation3,\n",
    "     \"Test\":test3}\n",
    "\n",
    "results1_change_stage_cr = pd.DataFrame(results_dict1)\n",
    "results2_change_stage_cr = pd.DataFrame(results_dict2)\n",
    "results3_change_stage_cr = pd.DataFrame(results_dict3)\n",
    "\n",
    "results1_change_stage_cr.to_csv(\"./results/results_convnext_change_stage_cr_1.csv\",index=False)\n",
    "results2_change_stage_cr.to_csv(\"./results/results_convnext_change_stage_cr_2.csv\",index=False)\n",
    "results3_change_stage_cr.to_csv(\"./results/results_convnext_change_stage_cr_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_cr = (results1_change_stage_cr[\"Test\"].max() + results2_change_stage_cr[\"Test\"].max() + results3_change_stage_cr[\"Test\"].max())/3\n",
    "print(f\"Accuracy del modelo al cambiar la proporción de los stages: {accuracy_cr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3f1ce",
   "metadata": {},
   "source": [
    "## Cambiar *Stem* por *Patchify*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d11fd",
   "metadata": {},
   "source": [
    "Las redes ResNet empiezan con un bloque *Stem*, éste se encarga de aplicar un *downsampling* agresivo a las imágenes de entrada. *Patchify*, por otro lado, consiste en dividir en bloques a las imágenes de entrada, mediante capas convolucionales cuyo stride es igual al tamaño del kernel.\n",
    "\n",
    "En el modelo particular que hemos estado usando optamos por evitar usar *downsampling* en el bloque *Stem* debido a que la dimensión de nuestras imágenes es más pequeña que la de las imágenes de ImageNet para las que fue planteado este diseño. Por ello mismo, hemos optado por utilizar *patchify* de dimensión 1, para evitar reducir el tamaño de las imágenes. Originalmente se utiliza *patchify* de dimensión 4.\n",
    "\n",
    "Para implementar esto, redefinimos el bloque Stem para que consista de una capa convolucional seguida de una capa de Batch Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f30bafbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stem(nn.Sequential):\n",
    "    def __init__(self, in_features, out_features, patch_size=1):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_features, out_features, kernel_size=patch_size, stride=patch_size),\n",
    "            nn.BatchNorm2d(out_features) \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178218a6",
   "metadata": {},
   "source": [
    "#### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0d233d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training accuracy: 0.196, Validation accuracy: 0.19, loss = 2.384\n",
      "Time spent on epoch 1: 4.03min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model2 \u001b[38;5;241m=\u001b[39m ResNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, stem_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, depths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m2\u001b[39m], widths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m512\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m model3 \u001b[38;5;241m=\u001b[39m ResNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, stem_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, depths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m2\u001b[39m], widths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m512\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 6\u001b[0m model1, training1, validation1, test1, loss1 \u001b[38;5;241m=\u001b[39m \u001b[43mentrenamiento\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model2, training2, validation2, test2, loss2 \u001b[38;5;241m=\u001b[39m entrenamiento(model2, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      8\u001b[0m model3, training3, validation3, test3, loss3 \u001b[38;5;241m=\u001b[39m entrenamiento(model3, \u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 47\u001b[0m, in \u001b[0;36mentrenamiento\u001b[1;34m(model, epocas)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m images, labels, outputs\n\u001b[0;32m     46\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m---> 47\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m     49\u001b[0m loss_epoch\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;66;03m# Guardar la información del loss de esta época\u001b[39;00m\n\u001b[0;32m     50\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Implementación de learning rate decay\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Repetimos 3 veces el experimento\n",
    "model1 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model2 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model3 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "\n",
    "model1, training1, validation1, test1, loss1 = entrenamiento(model1, 100)\n",
    "model2, training2, validation2, test2, loss2 = entrenamiento(model2, 100)\n",
    "model3, training3, validation3, test3, loss3 = entrenamiento(model3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8016f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados\n",
    "results_dict1 = {\"loss\": loss1,\n",
    "    'Train':training1,\n",
    "     'Validation': validation1,\n",
    "     \"Test\":test1}\n",
    "results_dict2 = {\"loss\": loss2,\n",
    "    'Train':training2,\n",
    "     'Validation': validation2,\n",
    "     \"Test\":test2}\n",
    "results_dict3 = {\"loss\": loss3,\n",
    "    'Train':training3,\n",
    "     'Validation': validation3,\n",
    "     \"Test\":test3}\n",
    "\n",
    "results1_patchify = pd.DataFrame(results_dict1)\n",
    "results2_patchify = pd.DataFrame(results_dict2)\n",
    "results3_patchify = pd.DataFrame(results_dict3)\n",
    "\n",
    "results1_patchify.to_csv(\"./results/results_convnext_patchify_1.csv\",index=False)\n",
    "results2_patchify.to_csv(\"./results/results_convnext_patchify_2.csv\",index=False)\n",
    "results3_patchify.to_csv(\"./results/results_convnext_patchify_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfe85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_patch = (results1_patchify[\"Test\"].max() + results2_patchify[\"Test\"].max() + results3_patchify[\"Test\"].max())/3\n",
    "print(f\"Accuracy del modelo al aplicar Patchify: {accuracy_patch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ae242",
   "metadata": {},
   "source": [
    "## Añadir *depthwise convolution* (*ResNeXt-ify*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb149bc8",
   "metadata": {},
   "source": [
    "Basados en el artículo [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/abs/1611.05431), se propone utilizar *grouped convolution* en la capa convolucional en la que el kernel es de tamaño 3 del bloque Bottleneck . En particular, se utiliza un tipo de *grouped convolution* llamdo *depthwise*, en donde el número de grupos es el mismo que el número de canales de entrada.\n",
    "\n",
    "Para implementar este cambio basta utilizar el parámetro *groups* al llamar a la capa nn.Conv2d() en cuestión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3d4a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(self,in_features, out_features, reduction = 4, stride = 1):\n",
    "        super().__init__()\n",
    "        reduced_features = out_features // reduction\n",
    "        self.block = nn.Sequential(\n",
    "            # Reducción de canales\n",
    "            utilConv(in_features, reduced_features, kernel_size=1, stride=stride, bias=False),\n",
    "            # El número de canales se mantiene fijo\n",
    "            utilConv(reduced_features, reduced_features, kernel_size=3, bias=False, groups=reduced_features), # en esta capa se utiliza grouped convolution\n",
    "            # Aumento de canales\n",
    "            utilConv(reduced_features, out_features, kernel_size=1, bias=False, act=nn.Identity), \n",
    "        )\n",
    "        \n",
    "        # self.shortcut es utilizado para transformar al input a las dimensiones correctas para poder sumarlo a la salida del bloque\n",
    "        if in_features != out_features:\n",
    "            self.shortcut =nn.Sequential(utilConv(in_features, out_features, kernel_size=1, stride=stride, bias=False))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        res = self.shortcut(res)\n",
    "        x += res\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d93bb",
   "metadata": {},
   "source": [
    "#### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5cefe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training accuracy: 0.245, Validation accuracy: 0.246, loss = 2.307\n",
      "Time spent on epoch 1: 3.39min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model2 \u001b[38;5;241m=\u001b[39m ResNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, stem_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, depths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m2\u001b[39m], widths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m512\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m model3 \u001b[38;5;241m=\u001b[39m ResNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, stem_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, depths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m2\u001b[39m], widths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m512\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 6\u001b[0m model1, training1, validation1, test1, loss1 \u001b[38;5;241m=\u001b[39m \u001b[43mentrenamiento\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model2, training2, validation2, test2, loss2 \u001b[38;5;241m=\u001b[39m entrenamiento(model2, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      8\u001b[0m model3, training3, validation3, test3, loss3 \u001b[38;5;241m=\u001b[39m entrenamiento(model3, \u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 46\u001b[0m, in \u001b[0;36mentrenamiento\u001b[1;34m(model, epocas)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Ahorro de memoria\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m images, labels, outputs\n\u001b[1;32m---> 46\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m     49\u001b[0m loss_epoch\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;66;03m# Guardar la información del loss de esta época\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Repetimos 3 veces el experimento\n",
    "model1 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model2 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model3 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "\n",
    "model1, training1, validation1, test1, loss1 = entrenamiento(model1, 100)\n",
    "model2, training2, validation2, test2, loss2 = entrenamiento(model2, 100)\n",
    "model3, training3, validation3, test3, loss3 = entrenamiento(model3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c678af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados\n",
    "results_dict1 = {\"loss\": loss1,\n",
    "    'Train':training1,\n",
    "     'Validation': validation1,\n",
    "     \"Test\":test1}\n",
    "results_dict2 = {\"loss\": loss2,\n",
    "    'Train':training2,\n",
    "     'Validation': validation2,\n",
    "     \"Test\":test2}\n",
    "results_dict3 = {\"loss\": loss3,\n",
    "    'Train':training3,\n",
    "     'Validation': validation3,\n",
    "     \"Test\":test3}\n",
    "\n",
    "results1_resnextify = pd.DataFrame(results_dict1)\n",
    "results2_resnextify = pd.DataFrame(results_dict2)\n",
    "results3_resnextify = pd.DataFrame(results_dict3)\n",
    "\n",
    "results1_resnextify.to_csv(\"./results/results_convnext_resnextify_1.csv\",index=False)\n",
    "results2_resnextify.to_csv(\"./results/results_convnext_resnextify_2.csv\",index=False)\n",
    "results3_resnextify.to_csv(\"./results/results_convnext_resnextify_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed95c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_resnext = (results1_resnextify[\"Test\"].max() + results2_resnextify[\"Test\"].max() + results3_resnextify[\"Test\"].max())/3\n",
    "print(f\"Accuracy del modelo al aplicar  ResNeXt-ify: {accuracy_resnext}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623093cf",
   "metadata": {},
   "source": [
    "## Inverted-Bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a74ddc",
   "metadata": {},
   "source": [
    "El modelo ResNet-50 utiliza el bloque Bottleneck. El nombre de dicho bloque se debe a que primero reduce el número de canales mediante una convolución con kernel de tamaño 1, después mantiene el número de canales y aplica una convolución con kernel de tamaño 3 y por último aumenta el número de canales al original.\n",
    "\n",
    "Los transformadores de visión utilizan un sistema opuesto. Primero aumentan el número de canales, después aplican la convolución con kernel de dimensión mayor a 1 y por último reducen el número de canales al original. Este tipo de bloque se llama *Inverted-Bottleneck*, o Bottleneck invertido.\n",
    "\n",
    "Inspirados en el diseño de los transformadores de visión, se propone utilizar bloques Bottleneck invertidos. La implementación se puede llevar a cabo mediante la modificación del bloque BottleNeck como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a13447ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, expansion = 4, stride = 1):\n",
    "        super().__init__()\n",
    "        expanded_features = out_features * expansion\n",
    "        self.block = nn.Sequential(\n",
    "            # Aumento de canales\n",
    "            utilConv(in_features, expanded_features, kernel_size=1, stride=stride, bias=False),\n",
    "            # El número de canales se mantiene fijo (Aquí se aplica la convolución depthwise)\n",
    "            utilConv(expanded_features, expanded_features, kernel_size=3, bias=False, groups=in_features),\n",
    "            # Reducción de canales\n",
    "            utilConv(expanded_features, out_features, kernel_size=1, bias=False, act=nn.Identity)\n",
    "        )\n",
    "\n",
    "        # self.shortcut es utilizado para transformar al input a las dimensiones correctas para poder sumarlo a la salida del bloque\n",
    "        if in_features != out_features:\n",
    "            self.shortcut =nn.Sequential(utilConv(in_features, out_features, kernel_size=1, stride=stride, bias=False))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        res = self.shortcut(res)\n",
    "        x += res\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adbfc26",
   "metadata": {},
   "source": [
    "#### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d0329ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training accuracy: 0.277, Validation accuracy: 0.277, loss = 2.018\n",
      "Time spent on epoch 1: 6.54min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model2 \u001b[38;5;241m=\u001b[39m ResNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, stem_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, depths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m2\u001b[39m], widths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m512\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m model3 \u001b[38;5;241m=\u001b[39m ResNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, stem_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, depths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m2\u001b[39m], widths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m512\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 6\u001b[0m model1, training1, validation1, test1, loss1 \u001b[38;5;241m=\u001b[39m \u001b[43mentrenamiento\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model2, training2, validation2, test2, loss2 \u001b[38;5;241m=\u001b[39m entrenamiento(model2, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      8\u001b[0m model3, training3, validation3, test3, loss3 \u001b[38;5;241m=\u001b[39m entrenamiento(model3, \u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 46\u001b[0m, in \u001b[0;36mentrenamiento\u001b[1;34m(model, epocas)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Ahorro de memoria\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m images, labels, outputs\n\u001b[1;32m---> 46\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m     49\u001b[0m loss_epoch\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;66;03m# Guardar la información del loss de esta época\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Repetimos 3 veces el experimento\n",
    "model1 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model2 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model3 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "\n",
    "model1, training1, validation1, test1, loss1 = entrenamiento(model1, 100)\n",
    "model2, training2, validation2, test2, loss2 = entrenamiento(model2, 100)\n",
    "model3, training3, validation3, test3, loss3 = entrenamiento(model3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7916482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados\n",
    "results_dict1 = {\"loss\": loss1,\n",
    "    'Train':training1,\n",
    "     'Validation': validation1,\n",
    "     \"Test\":test1}\n",
    "results_dict2 = {\"loss\": loss2,\n",
    "    'Train':training2,\n",
    "     'Validation': validation2,\n",
    "     \"Test\":test2}\n",
    "results_dict3 = {\"loss\": loss3,\n",
    "    'Train':training3,\n",
    "     'Validation': validation3,\n",
    "     \"Test\":test3}\n",
    "\n",
    "results1_inverted = pd.DataFrame(results_dict1)\n",
    "results2_inverted = pd.DataFrame(results_dict2)\n",
    "results3_inverted = pd.DataFrame(results_dict3)\n",
    "\n",
    "results1_inverted.to_csv(\"./results/results_convnext_inverted_1.csv\",index=False)\n",
    "results2_inverted.to_csv(\"./results/results_convnext_inverted_2.csv\",index=False)\n",
    "results3_inverted.to_csv(\"./results/results_convnext_inverted_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_inverted = (results1_inverted[\"Test\"].max() + results2_inverted[\"Test\"].max() + results3_inverted[\"Test\"].max())/3\n",
    "print(f\"Accuracy del modelo al invertir el cuello de botella: {accuracy_inverted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120245bd",
   "metadata": {},
   "source": [
    "## Aumentar el tamaño de los kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4983b3e7",
   "metadata": {},
   "source": [
    "Tomando como inspiración el modelo [Swin Transformers](https://arxiv.org/abs/2103.14030), se propone aumentar el tamaño del kernel de las capas convolucionales en el bloque Bottleneck, de 3x3 a 7x7. Para ello se plantea que es necesario hacer dos cosas:\n",
    "\n",
    "1. Mover la capa de *depthwise convolution* para que sea la primera del bloque\n",
    "2. Incrementar el tamaño del kernel de la capa de *depthwise convolution* a 7x7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f3de65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, expansion = 4, stride = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        expanded_features = out_features * expansion\n",
    "        self.block = nn.Sequential(\n",
    "            # El número de canales se mantiene fijo (con grupos depth-wise y kernel más grande)\n",
    "            utilConv(in_features, in_features, kernel_size=7, stride=stride, bias=False, groups=in_features),\n",
    "            # Aumento en el número de canales\n",
    "            utilConv(in_features, expanded_features, kernel_size=1),\n",
    "            # Reducción de canales\n",
    "            utilConv(expanded_features, out_features, kernel_size=1, bias=False, act=nn.Identity),\n",
    "        )\n",
    "        \n",
    "        # self.shortcut es utilizado para transformar al input a las dimensiones correctas para poder sumarlo a la salida del bloque\n",
    "        if in_features != out_features:\n",
    "            self.shortcut =nn.Sequential(utilConv(in_features, out_features, kernel_size=1, stride=stride, bias=False))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        res = self.shortcut(res)\n",
    "        x += res\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23703e78",
   "metadata": {},
   "source": [
    "#### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1757b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training accuracy: 0.321, Validation accuracy: 0.32, loss = 2.061\n",
      "Time spent on epoch 1: 4.73min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model2 \u001b[38;5;241m=\u001b[39m ResNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, stem_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, depths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m2\u001b[39m], widths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m512\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m model3 \u001b[38;5;241m=\u001b[39m ResNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, stem_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, depths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m2\u001b[39m], widths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m512\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 6\u001b[0m model1, training1, validation1, test1, loss1 \u001b[38;5;241m=\u001b[39m \u001b[43mentrenamiento\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model2, training2, validation2, test2, loss2 \u001b[38;5;241m=\u001b[39m entrenamiento(model2, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      8\u001b[0m model3, training3, validation3, test3, loss3 \u001b[38;5;241m=\u001b[39m entrenamiento(model3, \u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 29\u001b[0m, in \u001b[0;36mentrenamiento\u001b[1;34m(model, epocas)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     28\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     30\u001b[0m \n\u001b[0;32m     31\u001b[0m         \u001b[38;5;66;03m# Mover a los tensores a GPU de ser posible\u001b[39;00m\n\u001b[0;32m     32\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     33\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torchvision\\datasets\\cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torchvision\\transforms\\functional.py:170\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    169\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 170\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_num_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Repetimos 3 veces el experimento\n",
    "model1 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model2 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "model3 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[2,2,6,2], widths=[64, 128, 256,512]).to(device)\n",
    "\n",
    "model1, training1, validation1, test1, loss1 = entrenamiento(model1, 100)\n",
    "model2, training2, validation2, test2, loss2 = entrenamiento(model2, 100)\n",
    "model3, training3, validation3, test3, loss3 = entrenamiento(model3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e607d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados\n",
    "results_dict1 = {\"loss\": loss1,\n",
    "    'Train':training1,\n",
    "     'Validation': validation1,\n",
    "     \"Test\":test1}\n",
    "results_dict2 = {\"loss\": loss2,\n",
    "    'Train':training2,\n",
    "     'Validation': validation2,\n",
    "     \"Test\":test2}\n",
    "results_dict3 = {\"loss\": loss3,\n",
    "    'Train':training3,\n",
    "     'Validation': validation3,\n",
    "     \"Test\":test3}\n",
    "\n",
    "results1_kernel = pd.DataFrame(results_dict1)\n",
    "results2_kernel = pd.DataFrame(results_dict2)\n",
    "results3_kernel = pd.DataFrame(results_dict3)\n",
    "\n",
    "results1_kernel.to_csv(\"./results/results_convnext_kernel_1.csv\",index=False)\n",
    "results2_kernel.to_csv(\"./results/results_convnext_kernel_2.csv\",index=False)\n",
    "results3_kernel.to_csv(\"./results/results_convnext_kernel_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1269ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_kernel = (results1_kernel[\"Test\"].max() + results2_kernel[\"Test\"].max() + results3_kernel[\"Test\"].max())/3\n",
    "print(f\"Accuracy del modelo al aumentar el tamaño de los kernels: {accuracy_kernel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7aa59",
   "metadata": {},
   "source": [
    "## Cambios en el micro-diseño"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8796a8",
   "metadata": {},
   "source": [
    "Por último se proponen los siguientes cambios de menor escala en el diseño de la arquitectura:\n",
    "\n",
    "1. Cambiar la función de activación ReLU por GELU\n",
    "2. Disminuir el número de funciones de activación\n",
    "3. Disminución en el número de capas de normalización\n",
    "4. Cambiar *batch normalization* por *layer normalization*\n",
    "5. Separar las capas de *downsampling*\n",
    "6. Añadir *Stochastic Depth*\n",
    "\n",
    "Nuevamente, debido a que nuestras imágenes son de dimensión 32x32, en vez de 224x224 como fue propuesto en el artículo, optamos por omitir la primera capa de downsampling.\n",
    "\n",
    "Implementemos dichos cambios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "305fe736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerScaler(nn.Module):\n",
    "    def __init__(self, init_value, dimensions):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(init_value * torch.ones((dimensions)), \n",
    "                                    requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.gamma[None,...,None,None] * x\n",
    "\n",
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, expansion = 4, drop_p = .0, layer_scaler_init_value = 1e-6):\n",
    "        super().__init__()\n",
    "        expanded_features = out_features * expansion\n",
    "        self.block = nn.Sequential(\n",
    "            # narrow -> wide (with depth-wise and bigger kernel)\n",
    "            nn.Conv2d(\n",
    "                in_features, in_features, kernel_size=7, padding=3, bias=False, groups=in_features\n",
    "            ),\n",
    "            # GroupNorm with num_groups=1 is the same as LayerNorm but works for 2D data\n",
    "            nn.GroupNorm(num_groups=1, num_channels=in_features),\n",
    "            # wide -> wide \n",
    "            nn.Conv2d(in_features, expanded_features, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            # wide -> narrow\n",
    "            nn.Conv2d(expanded_features, out_features, kernel_size=1),\n",
    "        )\n",
    "        self.layer_scaler = LayerScaler(layer_scaler_init_value, out_features)\n",
    "        self.drop_path = StochasticDepth(drop_p, mode=\"batch\")\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        x = self.layer_scaler(x)\n",
    "        x = self.drop_path(x)\n",
    "        x += res\n",
    "        return x\n",
    "    \n",
    "class Stage(nn.Sequential):\n",
    "    def __init__(self, in_features, out_features, depth, drop_p = .0):\n",
    "        \n",
    "        if in_features != out_features:\n",
    "            super().__init__(\n",
    "                # Añadimos la capa de downsampling previo al stage.\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(num_groups=1, num_channels=in_features),\n",
    "                    nn.Conv2d(in_features, out_features, kernel_size=2, stride=2)\n",
    "                ),\n",
    "                # Añadimos los stages\n",
    "                *[BottleNeckBlock(out_features, out_features, drop_p=drop_p) for _ in range(depth)]\n",
    "            )\n",
    "        else:\n",
    "            super().__init__(\n",
    "                # Con esto garantizamos que no haya downsampling previo al primer stage\n",
    "                *[BottleNeckBlock(out_features, out_features, drop_p=drop_p) for _ in range(depth)]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a176af",
   "metadata": {},
   "source": [
    "#### Construimos el enconder y decoder de ConvNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24b7af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, stem_features, depths, widths, drop_p = .0):\n",
    "        super().__init__()\n",
    "        self.stem = Stem(in_channels, stem_features)\n",
    "\n",
    "        in_out_widths = list(zip(widths, widths[1:]))\n",
    "        # create drop paths probabilities (one for each stage)\n",
    "        drop_probs = [x.item() for x in torch.linspace(0, drop_p, sum(depths))]     \n",
    "        \n",
    "        self.stages = nn.ModuleList()\n",
    "        self.stages.append(Stage(stem_features, widths[0], depths[0], drop_p=drop_probs[0]))\n",
    "        for (in_features, out_features), depth, drop_p in zip(in_out_widths, depths[1:], drop_probs[1:]):\n",
    "            self.stages.append(Stage(in_features, out_features, depth, drop_p=drop_p))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49c1591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextDecoder(nn.Sequential):\n",
    "    def __init__(self, num_channels, num_classes = 10):\n",
    "        super().__init__(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(1),\n",
    "            nn.LayerNorm(num_channels),\n",
    "            nn.Linear(num_channels, num_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88580c8",
   "metadata": {},
   "source": [
    "#### Definición del modelo final Modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "438ef397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNext(nn.Sequential):\n",
    "    def __init__(self, in_channels, stem_features, depths, widths, drop_p = .0, num_classes = 10):\n",
    "        super().__init__()\n",
    "        self.encoder = ConvNextEncoder(in_channels, stem_features, depths, widths, drop_p)\n",
    "        self.head = ConvNextDecoder(widths[-1], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cec02557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 8,531,978\n"
     ]
    }
   ],
   "source": [
    "# Número de parámetros del modelo\n",
    "model = ConvNext(3,64,[2,2,6,2],[64, 128, 256, 512])\n",
    "print(\"Number of parameters: {:,}\".format(sum(p.numel() for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e87d14a",
   "metadata": {},
   "source": [
    "#### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c32e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repetimos 3 veces el experimento\n",
    "model1 = ConvNext(3,64,[2,2,6,2],[64, 128, 256, 512]).to(device)\n",
    "model2 = ConvNext(3,64,[2,2,6,2],[64, 128, 256, 512]).to(device)\n",
    "model3 = ConvNext(3,64,[2,2,6,2],[64, 128, 256, 512]).to(device)\n",
    "\n",
    "model1, training1, validation1, test1, loss1 = entrenamiento(model1, 100)\n",
    "model2, training2, validation2, test2, loss2 = entrenamiento(model2, 100)\n",
    "model3, training3, validation3, test3, loss3 = entrenamiento(model3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5b01a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados\n",
    "results_dict1 = {\"loss\": loss1,\n",
    "    'Train':training1,\n",
    "     'Validation': validation1,\n",
    "     \"Test\":test1}\n",
    "results_dict2 = {\"loss\": loss2,\n",
    "    'Train':training2,\n",
    "     'Validation': validation2,\n",
    "     \"Test\":test2}\n",
    "results_dict3 = {\"loss\": loss3,\n",
    "    'Train':training3,\n",
    "     'Validation': validation3,\n",
    "     \"Test\":test3}\n",
    "\n",
    "results1_final = pd.DataFrame(results_dict1)\n",
    "results2_final = pd.DataFrame(results_dict2)\n",
    "results3_final = pd.DataFrame(results_dict3)\n",
    "\n",
    "results1_final.to_csv(\"./results/results_convnext_final_1.csv\",index=False)\n",
    "results2_final.to_csv(\"./results/results_convnext_final_2.csv\",index=False)\n",
    "results3_final.to_csv(\"./results/results_convnext_final_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_final = (results1_final[\"Test\"].max() + results2_final[\"Test\"].max() + results3_final[\"Test\"].max())/3\n",
    "print(f\"Accuracy del modelo ConvNeXt: {accuracy_final}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
