{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8846f0e1",
   "metadata": {},
   "source": [
    "# Modelo ConvNeXt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b77232",
   "metadata": {},
   "source": [
    "% Explicacion de lo que se hará y cómo se hará"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2417ca",
   "metadata": {},
   "source": [
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c60cdb",
   "metadata": {},
   "source": [
    "Entrenaremos al modelo en el conjunto de datos CIFAR-10. Este dataset consiste de 60000 imágenes a color en 10 clases distintas, donde no hay intersección entre las distintas clases. Se puede acceder al dataset mediante las herramientas de la paquetería de pytorch, o también en la página oficial: https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee7b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las paqueterías necesarias para el notebook\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# De ser posible utilizaremos GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a945aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def data_loader(data_dir,\n",
    "                batch_size,\n",
    "                random_seed=42,\n",
    "                valid_size=0.1,\n",
    "                shuffle=True,\n",
    "                test=False):\n",
    "    \"\"\"\n",
    "    Función para cargar los datos de CIFAR-10\n",
    "    \"\"\"\n",
    "    \n",
    "    # Definimos el transform para normalizar los datos con pytorch\n",
    "    # Los valores fueron obtenidos en el notebook de datos \"data_extraction.ipynb\"\n",
    "    normalize = transforms.Normalize(  \n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # Definimos el transform para preporcesar los datos\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),  # Tomo imágenes de 34x34 para evitar una excepción en pytorch al normalizar la última capa\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    # Obtener los datos del conjunto de prueba\n",
    "    if test:\n",
    "        dataset = datasets.CIFAR10(\n",
    "          root=data_dir, train=False,\n",
    "          download=True, transform=transform_test,\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    # Cargamos una copia de los datos de entrenamiento\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform_train,\n",
    "    )\n",
    "    \n",
    "    # Cargamos una copia extra de los datos de entrenamiento para dividirlo después en el conjunto de validación\n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform_train,\n",
    "    )\n",
    "    \n",
    "    # Separamos los datos de entrenamiento y validación mediante índices\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    # Finalmente, definimos los conjuntos de entrenamiento y validación\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "\n",
    "# Aplicamos la función para cargar los datos de CIFAR-10, los guardamos en el directorio actual\n",
    "train_loader, valid_loader = data_loader(data_dir='./data',\n",
    "                                         batch_size=64)\n",
    "\n",
    "test_loader = data_loader(data_dir='./data',\n",
    "                              batch_size=64,\n",
    "                              test=True)    \n",
    "cifar10_classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b48f00",
   "metadata": {},
   "source": [
    "### Función de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d597e94",
   "metadata": {},
   "source": [
    "Con la siguiente función obtendremos los resultados de cada modelo, explicar que los datos salen de la primera parte de modificación del paper de convnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19223e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def entrenamiento(model, epocas):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # variables para guardar los resultados\n",
    "    accuracy_training_epochs = []\n",
    "    accuracy_validation_epochs = []\n",
    "    loss_epoch = []\n",
    "    test_accuracy = []\n",
    "    \n",
    "    # parámetros de entrenamiento\n",
    "    num_epochs = epocas\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(),\n",
    "                            lr=0.004,\n",
    "                            betas=(0.9, 0.999),\n",
    "                            weight_decay=0.05\n",
    "                            )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epocas)\n",
    "\n",
    "    \n",
    "    # entrenamiento\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "            # Mover a los tensores a GPU de ser posible\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Ahorro de memoria\n",
    "            del images, labels, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        loss_epoch.append(loss.item()) # Guardar la información del loss de esta época\n",
    "        lr_scheduler.step() # Implementación de learning rate decay\n",
    "\n",
    "        # Medición de la exactitud en el conjunto de validación\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in valid_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                del images, labels, outputs\n",
    "            val_accuracy = correct/total\n",
    "            accuracy_validation_epochs.append(val_accuracy)\n",
    "\n",
    "        # Medición de la exactitud sobre todo el conjunto de entrenamiento\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in train_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                del images, labels, outputs\n",
    "            train_accuracy = correct/total\n",
    "            accuracy_training_epochs.append(train_accuracy)\n",
    "\n",
    "        # Medición de la exactitud en el conjunto de prueba\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                del images, labels, outputs\n",
    "            t_acc = correct/total\n",
    "            test_accuracy.append(t_acc)\n",
    "\n",
    "\n",
    "        # Imprimir la pérdida, la exactitud en la validación y la exactitud en los datos de entrenamiento, de esta época.\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training accuracy: {round(train_accuracy,3)}, Validation accuracy: {round(val_accuracy,3)}, loss = {round(loss_epoch[-1],3)}\")\n",
    "        print(f\"Time spent on epoch {epoch+1}: {round((time.time()-start_time)/60,2)}min\")\n",
    "        \n",
    "   # return final model, training accuracy, validation accuracy, test accuracy, loss. Info de todas las épocas.     \n",
    "    return [model,\n",
    "            accuracy_training_epochs,\n",
    "            accuracy_validation_epochs,\n",
    "            test_accuracy,\n",
    "            loss_epoch]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ebd15",
   "metadata": {},
   "source": [
    "\n",
    "### ResNet-50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5a9d2e",
   "metadata": {},
   "source": [
    "El paper parte de una ResNet-50 como modelo base al que irá modificndo.\n",
    "La explicación del código se encuentra en el notebook de ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1301d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class utilConv(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Capa de utilidad compuesta por una capa de convolución seguida de una de normalización y luego una de activación.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, kernel_size, stride = 1, norm = nn.BatchNorm2d, act = nn.ReLU, bias=True):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_features, out_features, kernel_size=kernel_size ,padding=kernel_size // 2, stride=stride, bias=bias),\n",
    "            norm(out_features),\n",
    "            act()\n",
    "        )\n",
    "        \n",
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(self,in_features, out_features, reduction = 4, stride = 1):\n",
    "        super().__init__()\n",
    "        reduced_features = out_features // reduction\n",
    "        self.block = nn.Sequential(\n",
    "            # Reducción de canales\n",
    "            utilConv(in_features, reduced_features, kernel_size=1, stride=stride, bias=False), # el stride puede ser 2 para aplicar downsampling\n",
    "            # El número de canales se mantiene fijo\n",
    "            utilConv(reduced_features, reduced_features, kernel_size=3, bias=False),\n",
    "            # Aumento de canales\n",
    "            utilConv(reduced_features, out_features, kernel_size=1, bias=False, act=nn.Identity), \n",
    "        )\n",
    "        \n",
    "        # self.shortcut es utilizado para transformar al input a las dimensiones correctas para poder sumarlo a la salida del bloque\n",
    "        if in_features != out_features:\n",
    "            self.shortcut =nn.Sequential(utilConv(in_features, out_features, kernel_size=1, stride=stride, bias=False))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        res = self.shortcut(res)\n",
    "        x += res\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "    \n",
    "class Stage(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Esta capa define al \"stage\", el cual es un conjunto de bloques residuales\n",
    "    depth es el número de bloques residuales\n",
    "    in_features el numero de canales con que empieza\n",
    "    out_features el numer de canales con que termina\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, depth, stride = 2):  # in_features y out_features deben ser distintos, sino se aplicará downsampling y el Bottleneck no aplicará la identidad\n",
    "        super().__init__(\n",
    "            \n",
    "            BottleNeckBlock(in_features, out_features, stride=stride), # Aquí se lleva a cabo el downsampling\n",
    "            *[BottleNeckBlock(out_features, out_features) for _ in range(depth - 1)]\n",
    "        )\n",
    "        \n",
    "        \n",
    "class Stem(nn.Sequential):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__(\n",
    "            utilConv(in_features, out_features, kernel_size=3, stride=1),  # en el caso de ImageNet, el kernel es de tamaño 7\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  \n",
    "        )\n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    widths es una lista con el número de canales respectivos al final de cada \"stage\"\n",
    "    depths es una lista con el numero de bloques residuales que tendrá cada \"stage\"\n",
    "    stem_features es el número de canales que resulta de la primera capa de downsampling\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, stem_features, depths, widths):  # \n",
    "        super().__init__()\n",
    "        self.stem = Stem(in_channels, stem_features)\n",
    "\n",
    "        in_out_widths = list(zip(widths, widths[1:]))\n",
    "\n",
    "        \n",
    "        self.stages = nn.ModuleList() # lista de pytorch con los stages\n",
    "        \n",
    "        self.stages.append(Stage(stem_features, widths[0], depths[0], stride=1)) # se puede inferir de la figura 1 del artículo que el primer bloque del stage1 tiene stride 1\n",
    "        \n",
    "        for (in_features, out_features), depth in zip(in_out_widths, depths[1:]):\n",
    "            # añadir cada uno de los stages\n",
    "            self.stages.append(Stage(in_features, out_features, depth))\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        for stage in self.stages:\n",
    "\n",
    "            x = stage(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Capa para clasificar los datos\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.decoder = nn.Linear(in_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.decoder(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53173e82",
   "metadata": {},
   "source": [
    "Con esto podemos definir nuestro modelo base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0adc4783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, n_classes, stem_features, depths, widths ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_channels=in_channels, stem_features=stem_features, depths=depths, widths=widths)\n",
    "        self.decoder = Decoder(widths[-1], n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de8bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Training accuracy: 0.252, Validation accuracy: 0.257, loss = 2.228\n",
      "Time spent on epoch 1: 3.13min\n",
      "Epoch [2/200], Training accuracy: 0.275, Validation accuracy: 0.271, loss = 2.324\n",
      "Time spent on epoch 2: 3.25min\n",
      "Epoch [3/200], Training accuracy: 0.321, Validation accuracy: 0.319, loss = 2.2\n",
      "Time spent on epoch 3: 3.33min\n",
      "Epoch [4/200], Training accuracy: 0.342, Validation accuracy: 0.335, loss = 2.332\n",
      "Time spent on epoch 4: 3.22min\n",
      "Epoch [5/200], Training accuracy: 0.375, Validation accuracy: 0.369, loss = 2.23\n",
      "Time spent on epoch 5: 3.05min\n",
      "Epoch [6/200], Training accuracy: 0.39, Validation accuracy: 0.392, loss = 2.17\n",
      "Time spent on epoch 6: 3.1min\n",
      "Epoch [7/200], Training accuracy: 0.41, Validation accuracy: 0.405, loss = 2.076\n",
      "Time spent on epoch 7: 3.02min\n",
      "Epoch [8/200], Training accuracy: 0.423, Validation accuracy: 0.419, loss = 2.204\n",
      "Time spent on epoch 8: 3.02min\n",
      "Epoch [9/200], Training accuracy: 0.469, Validation accuracy: 0.459, loss = 1.752\n",
      "Time spent on epoch 9: 3.02min\n",
      "Epoch [10/200], Training accuracy: 0.488, Validation accuracy: 0.483, loss = 2.049\n",
      "Time spent on epoch 10: 3.02min\n",
      "Epoch [11/200], Training accuracy: 0.51, Validation accuracy: 0.501, loss = 2.118\n",
      "Time spent on epoch 11: 3.02min\n",
      "Epoch [12/200], Training accuracy: 0.541, Validation accuracy: 0.533, loss = 1.876\n",
      "Time spent on epoch 12: 3.0min\n",
      "Epoch [13/200], Training accuracy: 0.521, Validation accuracy: 0.512, loss = 1.989\n",
      "Time spent on epoch 13: 3.02min\n",
      "Epoch [14/200], Training accuracy: 0.568, Validation accuracy: 0.571, loss = 2.164\n",
      "Time spent on epoch 14: 3.02min\n",
      "Epoch [15/200], Training accuracy: 0.562, Validation accuracy: 0.559, loss = 1.736\n",
      "Time spent on epoch 15: 3.01min\n",
      "Epoch [16/200], Training accuracy: 0.586, Validation accuracy: 0.575, loss = 1.617\n",
      "Time spent on epoch 16: 3.01min\n",
      "Epoch [17/200], Training accuracy: 0.593, Validation accuracy: 0.581, loss = 2.073\n",
      "Time spent on epoch 17: 3.01min\n",
      "Epoch [18/200], Training accuracy: 0.623, Validation accuracy: 0.608, loss = 2.301\n",
      "Time spent on epoch 18: 3.01min\n",
      "Epoch [19/200], Training accuracy: 0.615, Validation accuracy: 0.608, loss = 1.893\n",
      "Time spent on epoch 19: 3.01min\n",
      "Epoch [20/200], Training accuracy: 0.616, Validation accuracy: 0.607, loss = 1.915\n",
      "Time spent on epoch 20: 3.01min\n",
      "Epoch [21/200], Training accuracy: 0.607, Validation accuracy: 0.591, loss = 2.167\n",
      "Time spent on epoch 21: 3.02min\n",
      "Epoch [22/200], Training accuracy: 0.61, Validation accuracy: 0.603, loss = 1.829\n",
      "Time spent on epoch 22: 3.0min\n",
      "Epoch [23/200], Training accuracy: 0.628, Validation accuracy: 0.619, loss = 2.089\n",
      "Time spent on epoch 23: 3.01min\n",
      "Epoch [24/200], Training accuracy: 0.634, Validation accuracy: 0.63, loss = 2.008\n",
      "Time spent on epoch 24: 3.0min\n",
      "Epoch [25/200], Training accuracy: 0.622, Validation accuracy: 0.613, loss = 1.858\n",
      "Time spent on epoch 25: 3.0min\n",
      "Epoch [26/200], Training accuracy: 0.645, Validation accuracy: 0.64, loss = 1.897\n",
      "Time spent on epoch 26: 3.0min\n",
      "Epoch [27/200], Training accuracy: 0.628, Validation accuracy: 0.628, loss = 1.812\n",
      "Time spent on epoch 27: 3.0min\n",
      "Epoch [28/200], Training accuracy: 0.635, Validation accuracy: 0.621, loss = 2.03\n",
      "Time spent on epoch 28: 3.01min\n",
      "Epoch [29/200], Training accuracy: 0.642, Validation accuracy: 0.632, loss = 1.713\n",
      "Time spent on epoch 29: 3.01min\n",
      "Epoch [30/200], Training accuracy: 0.645, Validation accuracy: 0.634, loss = 1.814\n",
      "Time spent on epoch 30: 3.0min\n",
      "Epoch [31/200], Training accuracy: 0.655, Validation accuracy: 0.648, loss = 2.024\n",
      "Time spent on epoch 31: 3.01min\n",
      "Epoch [32/200], Training accuracy: 0.637, Validation accuracy: 0.625, loss = 2.047\n",
      "Time spent on epoch 32: 3.01min\n",
      "Epoch [33/200], Training accuracy: 0.633, Validation accuracy: 0.627, loss = 1.958\n",
      "Time spent on epoch 33: 3.0min\n",
      "Epoch [34/200], Training accuracy: 0.657, Validation accuracy: 0.645, loss = 1.854\n",
      "Time spent on epoch 34: 3.01min\n",
      "Epoch [35/200], Training accuracy: 0.636, Validation accuracy: 0.626, loss = 1.808\n",
      "Time spent on epoch 35: 3.0min\n",
      "Epoch [36/200], Training accuracy: 0.66, Validation accuracy: 0.656, loss = 1.87\n",
      "Time spent on epoch 36: 3.05min\n",
      "Epoch [37/200], Training accuracy: 0.664, Validation accuracy: 0.651, loss = 2.069\n",
      "Time spent on epoch 37: 3.02min\n",
      "Epoch [38/200], Training accuracy: 0.647, Validation accuracy: 0.643, loss = 1.829\n",
      "Time spent on epoch 38: 3.01min\n",
      "Epoch [39/200], Training accuracy: 0.647, Validation accuracy: 0.646, loss = 1.868\n",
      "Time spent on epoch 39: 3.03min\n",
      "Epoch [40/200], Training accuracy: 0.667, Validation accuracy: 0.664, loss = 1.721\n",
      "Time spent on epoch 40: 3.02min\n",
      "Epoch [41/200], Training accuracy: 0.661, Validation accuracy: 0.661, loss = 2.097\n",
      "Time spent on epoch 41: 3.01min\n",
      "Epoch [42/200], Training accuracy: 0.667, Validation accuracy: 0.655, loss = 1.79\n",
      "Time spent on epoch 42: 3.01min\n",
      "Epoch [43/200], Training accuracy: 0.673, Validation accuracy: 0.663, loss = 1.63\n",
      "Time spent on epoch 43: 3.01min\n",
      "Epoch [44/200], Training accuracy: 0.686, Validation accuracy: 0.683, loss = 1.871\n",
      "Time spent on epoch 44: 3.01min\n",
      "Epoch [45/200], Training accuracy: 0.683, Validation accuracy: 0.672, loss = 1.884\n",
      "Time spent on epoch 45: 3.0min\n",
      "Epoch [46/200], Training accuracy: 0.684, Validation accuracy: 0.675, loss = 1.777\n",
      "Time spent on epoch 46: 3.01min\n",
      "Epoch [47/200], Training accuracy: 0.679, Validation accuracy: 0.679, loss = 1.95\n",
      "Time spent on epoch 47: 3.01min\n",
      "Epoch [48/200], Training accuracy: 0.687, Validation accuracy: 0.678, loss = 1.807\n",
      "Time spent on epoch 48: 3.0min\n",
      "Epoch [49/200], Training accuracy: 0.682, Validation accuracy: 0.683, loss = 2.124\n",
      "Time spent on epoch 49: 3.01min\n",
      "Epoch [50/200], Training accuracy: 0.684, Validation accuracy: 0.685, loss = 1.738\n",
      "Time spent on epoch 50: 3.0min\n",
      "Epoch [51/200], Training accuracy: 0.671, Validation accuracy: 0.661, loss = 1.966\n",
      "Time spent on epoch 51: 3.01min\n",
      "Epoch [52/200], Training accuracy: 0.673, Validation accuracy: 0.668, loss = 1.792\n",
      "Time spent on epoch 52: 3.01min\n",
      "Epoch [53/200], Training accuracy: 0.686, Validation accuracy: 0.671, loss = 1.73\n",
      "Time spent on epoch 53: 3.0min\n",
      "Epoch [54/200], Training accuracy: 0.703, Validation accuracy: 0.701, loss = 1.887\n",
      "Time spent on epoch 54: 3.01min\n",
      "Epoch [55/200], Training accuracy: 0.695, Validation accuracy: 0.679, loss = 1.85\n",
      "Time spent on epoch 55: 3.01min\n",
      "Epoch [56/200], Training accuracy: 0.701, Validation accuracy: 0.692, loss = 1.747\n",
      "Time spent on epoch 56: 3.01min\n",
      "Epoch [57/200], Training accuracy: 0.694, Validation accuracy: 0.686, loss = 2.066\n",
      "Time spent on epoch 57: 3.01min\n",
      "Epoch [58/200], Training accuracy: 0.694, Validation accuracy: 0.681, loss = 1.674\n",
      "Time spent on epoch 58: 3.01min\n",
      "Epoch [59/200], Training accuracy: 0.697, Validation accuracy: 0.694, loss = 1.514\n",
      "Time spent on epoch 59: 3.0min\n",
      "Epoch [60/200], Training accuracy: 0.691, Validation accuracy: 0.681, loss = 1.96\n",
      "Time spent on epoch 60: 3.01min\n",
      "Epoch [61/200], Training accuracy: 0.714, Validation accuracy: 0.708, loss = 2.058\n",
      "Time spent on epoch 61: 3.01min\n",
      "Epoch [62/200], Training accuracy: 0.723, Validation accuracy: 0.715, loss = 1.867\n",
      "Time spent on epoch 62: 3.0min\n",
      "Epoch [63/200], Training accuracy: 0.69, Validation accuracy: 0.675, loss = 1.878\n",
      "Time spent on epoch 63: 3.0min\n",
      "Epoch [64/200], Training accuracy: 0.696, Validation accuracy: 0.682, loss = 1.888\n",
      "Time spent on epoch 64: 3.01min\n",
      "Epoch [65/200], Training accuracy: 0.684, Validation accuracy: 0.678, loss = 1.603\n",
      "Time spent on epoch 65: 3.0min\n",
      "Epoch [66/200], Training accuracy: 0.717, Validation accuracy: 0.706, loss = 1.835\n",
      "Time spent on epoch 66: 3.0min\n",
      "Epoch [67/200], Training accuracy: 0.706, Validation accuracy: 0.697, loss = 1.947\n",
      "Time spent on epoch 67: 3.0min\n",
      "Epoch [68/200], Training accuracy: 0.702, Validation accuracy: 0.695, loss = 1.81\n",
      "Time spent on epoch 68: 3.0min\n",
      "Epoch [69/200], Training accuracy: 0.708, Validation accuracy: 0.692, loss = 1.919\n",
      "Time spent on epoch 69: 3.0min\n",
      "Epoch [70/200], Training accuracy: 0.69, Validation accuracy: 0.683, loss = 1.957\n",
      "Time spent on epoch 70: 3.0min\n",
      "Epoch [71/200], Training accuracy: 0.723, Validation accuracy: 0.711, loss = 1.697\n",
      "Time spent on epoch 71: 3.01min\n",
      "Epoch [72/200], Training accuracy: 0.705, Validation accuracy: 0.693, loss = 1.587\n",
      "Time spent on epoch 72: 3.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/200], Training accuracy: 0.701, Validation accuracy: 0.692, loss = 2.187\n",
      "Time spent on epoch 73: 3.0min\n",
      "Epoch [74/200], Training accuracy: 0.723, Validation accuracy: 0.71, loss = 2.156\n",
      "Time spent on epoch 74: 3.0min\n",
      "Epoch [75/200], Training accuracy: 0.718, Validation accuracy: 0.699, loss = 1.67\n",
      "Time spent on epoch 75: 3.02min\n",
      "Epoch [76/200], Training accuracy: 0.725, Validation accuracy: 0.714, loss = 1.549\n",
      "Time spent on epoch 76: 3.0min\n",
      "Epoch [77/200], Training accuracy: 0.721, Validation accuracy: 0.714, loss = 1.729\n",
      "Time spent on epoch 77: 3.0min\n",
      "Epoch [78/200], Training accuracy: 0.725, Validation accuracy: 0.722, loss = 2.076\n",
      "Time spent on epoch 78: 3.0min\n",
      "Epoch [79/200], Training accuracy: 0.722, Validation accuracy: 0.72, loss = 1.711\n",
      "Time spent on epoch 79: 3.0min\n",
      "Epoch [80/200], Training accuracy: 0.733, Validation accuracy: 0.719, loss = 1.82\n",
      "Time spent on epoch 80: 3.02min\n",
      "Epoch [81/200], Training accuracy: 0.727, Validation accuracy: 0.716, loss = 2.013\n",
      "Time spent on epoch 81: 3.01min\n",
      "Epoch [82/200], Training accuracy: 0.733, Validation accuracy: 0.72, loss = 1.608\n",
      "Time spent on epoch 82: 2.99min\n",
      "Epoch [83/200], Training accuracy: 0.71, Validation accuracy: 0.699, loss = 1.803\n",
      "Time spent on epoch 83: 3.0min\n",
      "Epoch [84/200], Training accuracy: 0.73, Validation accuracy: 0.715, loss = 1.84\n",
      "Time spent on epoch 84: 3.0min\n",
      "Epoch [85/200], Training accuracy: 0.739, Validation accuracy: 0.722, loss = 1.652\n",
      "Time spent on epoch 85: 3.0min\n",
      "Epoch [86/200], Training accuracy: 0.735, Validation accuracy: 0.717, loss = 1.82\n",
      "Time spent on epoch 86: 3.01min\n",
      "Epoch [87/200], Training accuracy: 0.727, Validation accuracy: 0.701, loss = 1.67\n",
      "Time spent on epoch 87: 3.03min\n",
      "Epoch [88/200], Training accuracy: 0.726, Validation accuracy: 0.705, loss = 1.744\n",
      "Time spent on epoch 88: 3.01min\n",
      "Epoch [89/200], Training accuracy: 0.752, Validation accuracy: 0.74, loss = 1.768\n",
      "Time spent on epoch 89: 3.02min\n",
      "Epoch [90/200], Training accuracy: 0.734, Validation accuracy: 0.724, loss = 1.876\n",
      "Time spent on epoch 90: 3.02min\n",
      "Epoch [91/200], Training accuracy: 0.749, Validation accuracy: 0.739, loss = 1.784\n",
      "Time spent on epoch 91: 3.02min\n",
      "Epoch [92/200], Training accuracy: 0.737, Validation accuracy: 0.726, loss = 1.816\n",
      "Time spent on epoch 92: 3.01min\n",
      "Epoch [93/200], Training accuracy: 0.747, Validation accuracy: 0.738, loss = 1.728\n",
      "Time spent on epoch 93: 3.01min\n",
      "Epoch [94/200], Training accuracy: 0.742, Validation accuracy: 0.735, loss = 1.586\n",
      "Time spent on epoch 94: 3.0min\n",
      "Epoch [95/200], Training accuracy: 0.744, Validation accuracy: 0.723, loss = 1.996\n",
      "Time spent on epoch 95: 3.0min\n",
      "Epoch [96/200], Training accuracy: 0.744, Validation accuracy: 0.736, loss = 1.702\n",
      "Time spent on epoch 96: 3.01min\n",
      "Epoch [97/200], Training accuracy: 0.741, Validation accuracy: 0.739, loss = 1.71\n",
      "Time spent on epoch 97: 3.01min\n",
      "Epoch [98/200], Training accuracy: 0.75, Validation accuracy: 0.732, loss = 1.619\n",
      "Time spent on epoch 98: 3.01min\n",
      "Epoch [99/200], Training accuracy: 0.735, Validation accuracy: 0.724, loss = 1.936\n",
      "Time spent on epoch 99: 3.13min\n",
      "Epoch [100/200], Training accuracy: 0.749, Validation accuracy: 0.736, loss = 1.606\n",
      "Time spent on epoch 100: 3.36min\n",
      "Epoch [101/200], Training accuracy: 0.752, Validation accuracy: 0.741, loss = 2.07\n",
      "Time spent on epoch 101: 3.1min\n",
      "Epoch [102/200], Training accuracy: 0.756, Validation accuracy: 0.741, loss = 1.607\n",
      "Time spent on epoch 102: 3.04min\n",
      "Epoch [103/200], Training accuracy: 0.747, Validation accuracy: 0.732, loss = 1.652\n",
      "Time spent on epoch 103: 3.1min\n",
      "Epoch [104/200], Training accuracy: 0.758, Validation accuracy: 0.742, loss = 1.601\n",
      "Time spent on epoch 104: 3.05min\n",
      "Epoch [105/200], Training accuracy: 0.758, Validation accuracy: 0.744, loss = 1.831\n",
      "Time spent on epoch 105: 3.11min\n",
      "Epoch [106/200], Training accuracy: 0.766, Validation accuracy: 0.749, loss = 1.846\n",
      "Time spent on epoch 106: 3.09min\n",
      "Epoch [107/200], Training accuracy: 0.759, Validation accuracy: 0.74, loss = 1.999\n",
      "Time spent on epoch 107: 3.09min\n",
      "Epoch [108/200], Training accuracy: 0.76, Validation accuracy: 0.744, loss = 1.632\n",
      "Time spent on epoch 108: 3.03min\n",
      "Epoch [109/200], Training accuracy: 0.764, Validation accuracy: 0.746, loss = 1.757\n",
      "Time spent on epoch 109: 3.11min\n",
      "Epoch [110/200], Training accuracy: 0.761, Validation accuracy: 0.749, loss = 1.753\n",
      "Time spent on epoch 110: 3.11min\n",
      "Epoch [111/200], Training accuracy: 0.765, Validation accuracy: 0.765, loss = 2.018\n",
      "Time spent on epoch 111: 3.12min\n",
      "Epoch [112/200], Training accuracy: 0.769, Validation accuracy: 0.752, loss = 1.813\n",
      "Time spent on epoch 112: 3.12min\n",
      "Epoch [113/200], Training accuracy: 0.759, Validation accuracy: 0.749, loss = 1.956\n",
      "Time spent on epoch 113: 3.12min\n",
      "Epoch [114/200], Training accuracy: 0.769, Validation accuracy: 0.755, loss = 1.638\n",
      "Time spent on epoch 114: 3.09min\n",
      "Epoch [115/200], Training accuracy: 0.777, Validation accuracy: 0.755, loss = 1.591\n",
      "Time spent on epoch 115: 3.11min\n",
      "Epoch [116/200], Training accuracy: 0.77, Validation accuracy: 0.745, loss = 1.943\n",
      "Time spent on epoch 116: 3.13min\n",
      "Epoch [117/200], Training accuracy: 0.77, Validation accuracy: 0.743, loss = 1.839\n",
      "Time spent on epoch 117: 3.13min\n",
      "Epoch [118/200], Training accuracy: 0.779, Validation accuracy: 0.759, loss = 2.196\n",
      "Time spent on epoch 118: 3.13min\n",
      "Epoch [119/200], Training accuracy: 0.773, Validation accuracy: 0.756, loss = 1.967\n",
      "Time spent on epoch 119: 3.12min\n",
      "Epoch [120/200], Training accuracy: 0.778, Validation accuracy: 0.763, loss = 1.715\n",
      "Time spent on epoch 120: 3.14min\n",
      "Epoch [121/200], Training accuracy: 0.775, Validation accuracy: 0.758, loss = 1.594\n",
      "Time spent on epoch 121: 3.18min\n",
      "Epoch [122/200], Training accuracy: 0.783, Validation accuracy: 0.762, loss = 1.714\n",
      "Time spent on epoch 122: 3.2min\n",
      "Epoch [123/200], Training accuracy: 0.784, Validation accuracy: 0.772, loss = 1.748\n",
      "Time spent on epoch 123: 3.18min\n",
      "Epoch [124/200], Training accuracy: 0.78, Validation accuracy: 0.756, loss = 1.885\n",
      "Time spent on epoch 124: 3.18min\n",
      "Epoch [125/200], Training accuracy: 0.778, Validation accuracy: 0.763, loss = 1.908\n",
      "Time spent on epoch 125: 3.21min\n",
      "Epoch [126/200], Training accuracy: 0.792, Validation accuracy: 0.771, loss = 1.768\n",
      "Time spent on epoch 126: 3.23min\n",
      "Epoch [127/200], Training accuracy: 0.775, Validation accuracy: 0.757, loss = 1.622\n",
      "Time spent on epoch 127: 3.21min\n",
      "Epoch [128/200], Training accuracy: 0.805, Validation accuracy: 0.784, loss = 1.589\n",
      "Time spent on epoch 128: 3.21min\n",
      "Epoch [129/200], Training accuracy: 0.79, Validation accuracy: 0.774, loss = 1.88\n",
      "Time spent on epoch 129: 3.21min\n",
      "Epoch [130/200], Training accuracy: 0.789, Validation accuracy: 0.769, loss = 1.83\n",
      "Time spent on epoch 130: 3.25min\n",
      "Epoch [131/200], Training accuracy: 0.802, Validation accuracy: 0.783, loss = 1.461\n",
      "Time spent on epoch 131: 3.22min\n",
      "Epoch [132/200], Training accuracy: 0.795, Validation accuracy: 0.774, loss = 1.464\n",
      "Time spent on epoch 132: 3.27min\n",
      "Epoch [133/200], Training accuracy: 0.791, Validation accuracy: 0.77, loss = 1.697\n",
      "Time spent on epoch 133: 3.21min\n",
      "Epoch [134/200], Training accuracy: 0.799, Validation accuracy: 0.779, loss = 1.64\n",
      "Time spent on epoch 134: 3.25min\n",
      "Epoch [135/200], Training accuracy: 0.797, Validation accuracy: 0.777, loss = 1.57\n",
      "Time spent on epoch 135: 3.24min\n",
      "Epoch [136/200], Training accuracy: 0.804, Validation accuracy: 0.787, loss = 1.868\n",
      "Time spent on epoch 136: 3.27min\n",
      "Epoch [137/200], Training accuracy: 0.81, Validation accuracy: 0.786, loss = 2.08\n",
      "Time spent on epoch 137: 3.23min\n",
      "Epoch [138/200], Training accuracy: 0.814, Validation accuracy: 0.791, loss = 1.654\n",
      "Time spent on epoch 138: 3.23min\n",
      "Epoch [139/200], Training accuracy: 0.813, Validation accuracy: 0.794, loss = 1.69\n",
      "Time spent on epoch 139: 3.23min\n",
      "Epoch [140/200], Training accuracy: 0.819, Validation accuracy: 0.796, loss = 1.696\n",
      "Time spent on epoch 140: 3.21min\n",
      "Epoch [141/200], Training accuracy: 0.809, Validation accuracy: 0.794, loss = 1.694\n",
      "Time spent on epoch 141: 3.21min\n",
      "Epoch [142/200], Training accuracy: 0.819, Validation accuracy: 0.8, loss = 1.65\n",
      "Time spent on epoch 142: 3.21min\n",
      "Epoch [143/200], Training accuracy: 0.813, Validation accuracy: 0.794, loss = 1.588\n",
      "Time spent on epoch 143: 3.19min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [144/200], Training accuracy: 0.815, Validation accuracy: 0.792, loss = 1.717\n",
      "Time spent on epoch 144: 3.21min\n",
      "Epoch [145/200], Training accuracy: 0.821, Validation accuracy: 0.791, loss = 1.461\n",
      "Time spent on epoch 145: 3.21min\n",
      "Epoch [146/200], Training accuracy: 0.828, Validation accuracy: 0.806, loss = 1.67\n",
      "Time spent on epoch 146: 3.22min\n",
      "Epoch [147/200], Training accuracy: 0.832, Validation accuracy: 0.804, loss = 1.586\n",
      "Time spent on epoch 147: 3.21min\n",
      "Epoch [148/200], Training accuracy: 0.83, Validation accuracy: 0.805, loss = 1.596\n",
      "Time spent on epoch 148: 3.24min\n",
      "Epoch [149/200], Training accuracy: 0.828, Validation accuracy: 0.801, loss = 1.603\n",
      "Time spent on epoch 149: 3.27min\n",
      "Epoch [150/200], Training accuracy: 0.825, Validation accuracy: 0.806, loss = 1.69\n",
      "Time spent on epoch 150: 3.22min\n",
      "Epoch [151/200], Training accuracy: 0.832, Validation accuracy: 0.817, loss = 1.633\n",
      "Time spent on epoch 151: 3.26min\n",
      "Epoch [152/200], Training accuracy: 0.832, Validation accuracy: 0.812, loss = 1.854\n",
      "Time spent on epoch 152: 3.21min\n",
      "Epoch [153/200], Training accuracy: 0.84, Validation accuracy: 0.821, loss = 1.667\n",
      "Time spent on epoch 153: 3.2min\n",
      "Epoch [154/200], Training accuracy: 0.845, Validation accuracy: 0.817, loss = 1.732\n",
      "Time spent on epoch 154: 3.3min\n",
      "Epoch [155/200], Training accuracy: 0.836, Validation accuracy: 0.816, loss = 1.537\n",
      "Time spent on epoch 155: 3.62min\n",
      "Epoch [156/200], Training accuracy: 0.844, Validation accuracy: 0.82, loss = 1.702\n",
      "Time spent on epoch 156: 3.11min\n",
      "Epoch [157/200], Training accuracy: 0.847, Validation accuracy: 0.818, loss = 1.726\n",
      "Time spent on epoch 157: 3.41min\n",
      "Epoch [158/200], Training accuracy: 0.851, Validation accuracy: 0.822, loss = 1.737\n",
      "Time spent on epoch 158: 3.11min\n",
      "Epoch [159/200], Training accuracy: 0.846, Validation accuracy: 0.819, loss = 1.471\n",
      "Time spent on epoch 159: 2.96min\n",
      "Epoch [160/200], Training accuracy: 0.854, Validation accuracy: 0.824, loss = 1.509\n",
      "Time spent on epoch 160: 2.95min\n",
      "Epoch [161/200], Training accuracy: 0.853, Validation accuracy: 0.826, loss = 1.533\n",
      "Time spent on epoch 161: 2.93min\n",
      "Epoch [162/200], Training accuracy: 0.858, Validation accuracy: 0.826, loss = 1.712\n",
      "Time spent on epoch 162: 2.94min\n",
      "Epoch [163/200], Training accuracy: 0.86, Validation accuracy: 0.824, loss = 1.588\n",
      "Time spent on epoch 163: 2.95min\n",
      "Epoch [164/200], Training accuracy: 0.861, Validation accuracy: 0.832, loss = 1.715\n",
      "Time spent on epoch 164: 2.97min\n",
      "Epoch [165/200], Training accuracy: 0.863, Validation accuracy: 0.838, loss = 1.82\n",
      "Time spent on epoch 165: 2.96min\n",
      "Epoch [166/200], Training accuracy: 0.866, Validation accuracy: 0.835, loss = 1.499\n",
      "Time spent on epoch 166: 2.98min\n",
      "Epoch [167/200], Training accuracy: 0.866, Validation accuracy: 0.831, loss = 1.773\n",
      "Time spent on epoch 167: 2.98min\n",
      "Epoch [168/200], Training accuracy: 0.872, Validation accuracy: 0.831, loss = 1.586\n",
      "Time spent on epoch 168: 2.99min\n",
      "Epoch [169/200], Training accuracy: 0.871, Validation accuracy: 0.833, loss = 2.054\n",
      "Time spent on epoch 169: 2.99min\n",
      "Epoch [170/200], Training accuracy: 0.874, Validation accuracy: 0.844, loss = 1.59\n",
      "Time spent on epoch 170: 2.99min\n",
      "Epoch [171/200], Training accuracy: 0.875, Validation accuracy: 0.835, loss = 1.843\n",
      "Time spent on epoch 171: 3.0min\n",
      "Epoch [172/200], Training accuracy: 0.877, Validation accuracy: 0.841, loss = 1.894\n",
      "Time spent on epoch 172: 3.01min\n",
      "Epoch [173/200], Training accuracy: 0.878, Validation accuracy: 0.84, loss = 1.544\n",
      "Time spent on epoch 173: 3.0min\n",
      "Epoch [174/200], Training accuracy: 0.882, Validation accuracy: 0.848, loss = 1.778\n",
      "Time spent on epoch 174: 3.0min\n",
      "Epoch [175/200], Training accuracy: 0.885, Validation accuracy: 0.843, loss = 1.595\n",
      "Time spent on epoch 175: 3.0min\n",
      "Epoch [176/200], Training accuracy: 0.883, Validation accuracy: 0.848, loss = 1.586\n",
      "Time spent on epoch 176: 3.01min\n",
      "Epoch [177/200], Training accuracy: 0.884, Validation accuracy: 0.851, loss = 1.71\n",
      "Time spent on epoch 177: 3.03min\n",
      "Epoch [178/200], Training accuracy: 0.886, Validation accuracy: 0.853, loss = 1.742\n",
      "Time spent on epoch 178: 3.0min\n",
      "Epoch [179/200], Training accuracy: 0.889, Validation accuracy: 0.847, loss = 1.593\n",
      "Time spent on epoch 179: 3.01min\n",
      "Epoch [180/200], Training accuracy: 0.891, Validation accuracy: 0.85, loss = 1.789\n",
      "Time spent on epoch 180: 3.0min\n",
      "Epoch [181/200], Training accuracy: 0.893, Validation accuracy: 0.848, loss = 1.701\n",
      "Time spent on epoch 181: 2.99min\n",
      "Epoch [182/200], Training accuracy: 0.891, Validation accuracy: 0.848, loss = 1.733\n",
      "Time spent on epoch 182: 3.01min\n",
      "Epoch [183/200], Training accuracy: 0.895, Validation accuracy: 0.843, loss = 1.739\n",
      "Time spent on epoch 183: 3.03min\n",
      "Epoch [184/200], Training accuracy: 0.897, Validation accuracy: 0.858, loss = 1.579\n",
      "Time spent on epoch 184: 3.01min\n",
      "Epoch [185/200], Training accuracy: 0.895, Validation accuracy: 0.856, loss = 1.462\n",
      "Time spent on epoch 185: 3.02min\n",
      "Epoch [186/200], Training accuracy: 0.897, Validation accuracy: 0.856, loss = 1.567\n",
      "Time spent on epoch 186: 3.01min\n",
      "Epoch [187/200], Training accuracy: 0.897, Validation accuracy: 0.856, loss = 1.826\n",
      "Time spent on epoch 187: 3.01min\n",
      "Epoch [188/200], Training accuracy: 0.898, Validation accuracy: 0.858, loss = 1.547\n",
      "Time spent on epoch 188: 3.01min\n",
      "Epoch [189/200], Training accuracy: 0.9, Validation accuracy: 0.859, loss = 1.572\n",
      "Time spent on epoch 189: 3.01min\n",
      "Epoch [190/200], Training accuracy: 0.899, Validation accuracy: 0.854, loss = 1.694\n",
      "Time spent on epoch 190: 3.01min\n",
      "Epoch [191/200], Training accuracy: 0.9, Validation accuracy: 0.859, loss = 1.461\n",
      "Time spent on epoch 191: 3.01min\n",
      "Epoch [192/200], Training accuracy: 0.9, Validation accuracy: 0.863, loss = 1.658\n",
      "Time spent on epoch 192: 3.04min\n",
      "Epoch [193/200], Training accuracy: 0.9, Validation accuracy: 0.852, loss = 1.463\n",
      "Time spent on epoch 193: 3.01min\n",
      "Epoch [194/200], Training accuracy: 0.901, Validation accuracy: 0.859, loss = 1.463\n",
      "Time spent on epoch 194: 3.02min\n",
      "Epoch [195/200], Training accuracy: 0.902, Validation accuracy: 0.862, loss = 1.862\n",
      "Time spent on epoch 195: 3.02min\n",
      "Epoch [196/200], Training accuracy: 0.9, Validation accuracy: 0.86, loss = 1.585\n",
      "Time spent on epoch 196: 3.04min\n",
      "Epoch [197/200], Training accuracy: 0.901, Validation accuracy: 0.863, loss = 1.834\n",
      "Time spent on epoch 197: 3.03min\n",
      "Epoch [198/200], Training accuracy: 0.902, Validation accuracy: 0.861, loss = 1.461\n",
      "Time spent on epoch 198: 3.01min\n",
      "Epoch [199/200], Training accuracy: 0.903, Validation accuracy: 0.858, loss = 1.581\n",
      "Time spent on epoch 199: 3.01min\n",
      "Epoch [200/200], Training accuracy: 0.901, Validation accuracy: 0.856, loss = 1.658\n",
      "Time spent on epoch 200: 3.02min\n",
      "Epoch [1/200], Training accuracy: 0.297, Validation accuracy: 0.298, loss = 2.078\n",
      "Time spent on epoch 1: 3.01min\n",
      "Epoch [2/200], Training accuracy: 0.332, Validation accuracy: 0.319, loss = 2.216\n",
      "Time spent on epoch 2: 3.03min\n",
      "Epoch [3/200], Training accuracy: 0.343, Validation accuracy: 0.336, loss = 1.696\n",
      "Time spent on epoch 3: 3.03min\n",
      "Epoch [4/200], Training accuracy: 0.33, Validation accuracy: 0.313, loss = 1.915\n",
      "Time spent on epoch 4: 3.01min\n",
      "Epoch [5/200], Training accuracy: 0.384, Validation accuracy: 0.377, loss = 1.915\n",
      "Time spent on epoch 5: 3.02min\n",
      "Epoch [6/200], Training accuracy: 0.422, Validation accuracy: 0.421, loss = 1.733\n",
      "Time spent on epoch 6: 3.01min\n",
      "Epoch [7/200], Training accuracy: 0.434, Validation accuracy: 0.424, loss = 2.096\n",
      "Time spent on epoch 7: 3.03min\n",
      "Epoch [8/200], Training accuracy: 0.438, Validation accuracy: 0.434, loss = 2.059\n",
      "Time spent on epoch 8: 3.03min\n",
      "Epoch [9/200], Training accuracy: 0.474, Validation accuracy: 0.465, loss = 2.291\n",
      "Time spent on epoch 9: 3.02min\n",
      "Epoch [10/200], Training accuracy: 0.492, Validation accuracy: 0.48, loss = 2.086\n",
      "Time spent on epoch 10: 3.03min\n",
      "Epoch [11/200], Training accuracy: 0.537, Validation accuracy: 0.53, loss = 2.248\n",
      "Time spent on epoch 11: 3.02min\n",
      "Epoch [12/200], Training accuracy: 0.526, Validation accuracy: 0.521, loss = 2.103\n",
      "Time spent on epoch 12: 3.02min\n",
      "Epoch [13/200], Training accuracy: 0.558, Validation accuracy: 0.547, loss = 1.714\n",
      "Time spent on epoch 13: 3.02min\n",
      "Epoch [14/200], Training accuracy: 0.557, Validation accuracy: 0.554, loss = 2.089\n",
      "Time spent on epoch 14: 3.02min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/200], Training accuracy: 0.576, Validation accuracy: 0.569, loss = 1.763\n",
      "Time spent on epoch 15: 3.04min\n",
      "Epoch [16/200], Training accuracy: 0.586, Validation accuracy: 0.569, loss = 2.089\n",
      "Time spent on epoch 16: 3.02min\n",
      "Epoch [17/200], Training accuracy: 0.593, Validation accuracy: 0.582, loss = 1.948\n",
      "Time spent on epoch 17: 3.02min\n",
      "Epoch [18/200], Training accuracy: 0.589, Validation accuracy: 0.583, loss = 2.11\n",
      "Time spent on epoch 18: 3.02min\n",
      "Epoch [19/200], Training accuracy: 0.597, Validation accuracy: 0.585, loss = 1.956\n",
      "Time spent on epoch 19: 3.02min\n",
      "Epoch [20/200], Training accuracy: 0.604, Validation accuracy: 0.594, loss = 1.955\n",
      "Time spent on epoch 20: 3.02min\n",
      "Epoch [21/200], Training accuracy: 0.621, Validation accuracy: 0.61, loss = 1.828\n",
      "Time spent on epoch 21: 3.02min\n",
      "Epoch [22/200], Training accuracy: 0.623, Validation accuracy: 0.607, loss = 1.77\n",
      "Time spent on epoch 22: 3.02min\n",
      "Epoch [23/200], Training accuracy: 0.621, Validation accuracy: 0.611, loss = 2.001\n",
      "Time spent on epoch 23: 3.02min\n",
      "Epoch [24/200], Training accuracy: 0.636, Validation accuracy: 0.616, loss = 2.217\n",
      "Time spent on epoch 24: 3.02min\n",
      "Epoch [25/200], Training accuracy: 0.642, Validation accuracy: 0.637, loss = 1.962\n",
      "Time spent on epoch 25: 3.02min\n",
      "Epoch [26/200], Training accuracy: 0.63, Validation accuracy: 0.622, loss = 2.051\n",
      "Time spent on epoch 26: 3.01min\n",
      "Epoch [27/200], Training accuracy: 0.653, Validation accuracy: 0.638, loss = 1.745\n",
      "Time spent on epoch 27: 3.02min\n",
      "Epoch [28/200], Training accuracy: 0.631, Validation accuracy: 0.632, loss = 1.999\n",
      "Time spent on epoch 28: 3.02min\n",
      "Epoch [29/200], Training accuracy: 0.631, Validation accuracy: 0.615, loss = 1.79\n",
      "Time spent on epoch 29: 3.02min\n",
      "Epoch [30/200], Training accuracy: 0.641, Validation accuracy: 0.625, loss = 1.972\n",
      "Time spent on epoch 30: 3.01min\n",
      "Epoch [31/200], Training accuracy: 0.646, Validation accuracy: 0.626, loss = 1.932\n",
      "Time spent on epoch 31: 3.06min\n",
      "Epoch [32/200], Training accuracy: 0.633, Validation accuracy: 0.631, loss = 1.586\n",
      "Time spent on epoch 32: 3.02min\n",
      "Epoch [33/200], Training accuracy: 0.662, Validation accuracy: 0.65, loss = 1.95\n",
      "Time spent on epoch 33: 3.0min\n",
      "Epoch [34/200], Training accuracy: 0.657, Validation accuracy: 0.651, loss = 1.846\n",
      "Time spent on epoch 34: 3.01min\n",
      "Epoch [35/200], Training accuracy: 0.654, Validation accuracy: 0.644, loss = 1.852\n",
      "Time spent on epoch 35: 3.02min\n",
      "Epoch [36/200], Training accuracy: 0.657, Validation accuracy: 0.657, loss = 1.876\n",
      "Time spent on epoch 36: 3.07min\n",
      "Epoch [37/200], Training accuracy: 0.651, Validation accuracy: 0.634, loss = 2.074\n",
      "Time spent on epoch 37: 3.08min\n",
      "Epoch [38/200], Training accuracy: 0.667, Validation accuracy: 0.654, loss = 1.869\n",
      "Time spent on epoch 38: 3.01min\n",
      "Epoch [39/200], Training accuracy: 0.66, Validation accuracy: 0.654, loss = 2.143\n",
      "Time spent on epoch 39: 3.01min\n",
      "Epoch [40/200], Training accuracy: 0.671, Validation accuracy: 0.663, loss = 1.719\n",
      "Time spent on epoch 40: 3.01min\n",
      "Epoch [41/200], Training accuracy: 0.664, Validation accuracy: 0.665, loss = 1.768\n",
      "Time spent on epoch 41: 3.01min\n",
      "Epoch [42/200], Training accuracy: 0.672, Validation accuracy: 0.662, loss = 2.02\n",
      "Time spent on epoch 42: 3.02min\n",
      "Epoch [43/200], Training accuracy: 0.665, Validation accuracy: 0.658, loss = 1.96\n",
      "Time spent on epoch 43: 3.02min\n",
      "Epoch [44/200], Training accuracy: 0.683, Validation accuracy: 0.669, loss = 1.897\n",
      "Time spent on epoch 44: 3.01min\n",
      "Epoch [45/200], Training accuracy: 0.685, Validation accuracy: 0.667, loss = 1.798\n",
      "Time spent on epoch 45: 3.0min\n",
      "Epoch [46/200], Training accuracy: 0.691, Validation accuracy: 0.678, loss = 2.11\n",
      "Time spent on epoch 46: 3.01min\n",
      "Epoch [47/200], Training accuracy: 0.685, Validation accuracy: 0.679, loss = 1.945\n",
      "Time spent on epoch 47: 3.02min\n",
      "Epoch [48/200], Training accuracy: 0.693, Validation accuracy: 0.685, loss = 1.872\n",
      "Time spent on epoch 48: 3.02min\n",
      "Epoch [49/200], Training accuracy: 0.701, Validation accuracy: 0.691, loss = 1.791\n",
      "Time spent on epoch 49: 3.01min\n",
      "Epoch [50/200], Training accuracy: 0.695, Validation accuracy: 0.68, loss = 1.716\n",
      "Time spent on epoch 50: 3.02min\n",
      "Epoch [51/200], Training accuracy: 0.699, Validation accuracy: 0.694, loss = 1.591\n",
      "Time spent on epoch 51: 3.01min\n",
      "Epoch [52/200], Training accuracy: 0.706, Validation accuracy: 0.702, loss = 2.053\n",
      "Time spent on epoch 52: 3.01min\n",
      "Epoch [53/200], Training accuracy: 0.694, Validation accuracy: 0.692, loss = 1.738\n",
      "Time spent on epoch 53: 3.01min\n",
      "Epoch [54/200], Training accuracy: 0.706, Validation accuracy: 0.697, loss = 1.613\n",
      "Time spent on epoch 54: 3.01min\n",
      "Epoch [55/200], Training accuracy: 0.702, Validation accuracy: 0.686, loss = 1.691\n",
      "Time spent on epoch 55: 3.01min\n",
      "Epoch [56/200], Training accuracy: 0.707, Validation accuracy: 0.69, loss = 1.59\n",
      "Time spent on epoch 56: 3.02min\n",
      "Epoch [57/200], Training accuracy: 0.693, Validation accuracy: 0.691, loss = 1.691\n",
      "Time spent on epoch 57: 3.02min\n",
      "Epoch [58/200], Training accuracy: 0.708, Validation accuracy: 0.693, loss = 1.714\n",
      "Time spent on epoch 58: 3.01min\n",
      "Epoch [59/200], Training accuracy: 0.715, Validation accuracy: 0.701, loss = 1.608\n",
      "Time spent on epoch 59: 3.01min\n",
      "Epoch [60/200], Training accuracy: 0.716, Validation accuracy: 0.699, loss = 1.791\n",
      "Time spent on epoch 60: 3.01min\n",
      "Epoch [61/200], Training accuracy: 0.694, Validation accuracy: 0.678, loss = 2.056\n",
      "Time spent on epoch 61: 3.01min\n",
      "Epoch [62/200], Training accuracy: 0.721, Validation accuracy: 0.718, loss = 1.863\n",
      "Time spent on epoch 62: 3.02min\n",
      "Epoch [63/200], Training accuracy: 0.719, Validation accuracy: 0.709, loss = 1.711\n",
      "Time spent on epoch 63: 3.02min\n",
      "Epoch [64/200], Training accuracy: 0.715, Validation accuracy: 0.711, loss = 1.695\n",
      "Time spent on epoch 64: 3.01min\n",
      "Epoch [65/200], Training accuracy: 0.718, Validation accuracy: 0.708, loss = 1.743\n",
      "Time spent on epoch 65: 3.01min\n",
      "Epoch [66/200], Training accuracy: 0.721, Validation accuracy: 0.713, loss = 1.915\n",
      "Time spent on epoch 66: 3.01min\n",
      "Epoch [67/200], Training accuracy: 0.719, Validation accuracy: 0.712, loss = 1.76\n",
      "Time spent on epoch 67: 3.02min\n",
      "Epoch [68/200], Training accuracy: 0.725, Validation accuracy: 0.702, loss = 1.6\n",
      "Time spent on epoch 68: 3.01min\n",
      "Epoch [69/200], Training accuracy: 0.717, Validation accuracy: 0.706, loss = 1.711\n",
      "Time spent on epoch 69: 3.01min\n",
      "Epoch [70/200], Training accuracy: 0.723, Validation accuracy: 0.719, loss = 1.719\n",
      "Time spent on epoch 70: 3.01min\n",
      "Epoch [71/200], Training accuracy: 0.722, Validation accuracy: 0.712, loss = 1.791\n",
      "Time spent on epoch 71: 3.03min\n",
      "Epoch [72/200], Training accuracy: 0.737, Validation accuracy: 0.723, loss = 1.954\n",
      "Time spent on epoch 72: 3.01min\n",
      "Epoch [73/200], Training accuracy: 0.723, Validation accuracy: 0.711, loss = 1.717\n",
      "Time spent on epoch 73: 3.0min\n",
      "Epoch [74/200], Training accuracy: 0.735, Validation accuracy: 0.719, loss = 1.823\n",
      "Time spent on epoch 74: 3.01min\n",
      "Epoch [75/200], Training accuracy: 0.729, Validation accuracy: 0.715, loss = 1.954\n",
      "Time spent on epoch 75: 3.03min\n",
      "Epoch [76/200], Training accuracy: 0.723, Validation accuracy: 0.709, loss = 1.873\n",
      "Time spent on epoch 76: 3.01min\n",
      "Epoch [77/200], Training accuracy: 0.741, Validation accuracy: 0.729, loss = 1.855\n",
      "Time spent on epoch 77: 3.0min\n",
      "Epoch [78/200], Training accuracy: 0.728, Validation accuracy: 0.721, loss = 1.962\n",
      "Time spent on epoch 78: 3.01min\n",
      "Epoch [79/200], Training accuracy: 0.728, Validation accuracy: 0.726, loss = 1.596\n",
      "Time spent on epoch 79: 3.0min\n",
      "Epoch [80/200], Training accuracy: 0.732, Validation accuracy: 0.724, loss = 1.734\n",
      "Time spent on epoch 80: 2.99min\n",
      "Epoch [81/200], Training accuracy: 0.737, Validation accuracy: 0.718, loss = 1.898\n",
      "Time spent on epoch 81: 2.99min\n",
      "Epoch [82/200], Training accuracy: 0.74, Validation accuracy: 0.722, loss = 1.837\n",
      "Time spent on epoch 82: 3.01min\n",
      "Epoch [83/200], Training accuracy: 0.731, Validation accuracy: 0.727, loss = 1.809\n",
      "Time spent on epoch 83: 3.01min\n",
      "Epoch [84/200], Training accuracy: 0.745, Validation accuracy: 0.723, loss = 1.7\n",
      "Time spent on epoch 84: 2.99min\n",
      "Epoch [85/200], Training accuracy: 0.743, Validation accuracy: 0.731, loss = 1.711\n",
      "Time spent on epoch 85: 3.0min\n",
      "Epoch [86/200], Training accuracy: 0.742, Validation accuracy: 0.731, loss = 1.764\n",
      "Time spent on epoch 86: 2.98min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [87/200], Training accuracy: 0.742, Validation accuracy: 0.731, loss = 1.844\n",
      "Time spent on epoch 87: 2.9min\n",
      "Epoch [88/200], Training accuracy: 0.738, Validation accuracy: 0.721, loss = 1.709\n",
      "Time spent on epoch 88: 2.89min\n",
      "Epoch [89/200], Training accuracy: 0.756, Validation accuracy: 0.742, loss = 1.908\n",
      "Time spent on epoch 89: 2.87min\n",
      "Epoch [90/200], Training accuracy: 0.756, Validation accuracy: 0.745, loss = 1.596\n",
      "Time spent on epoch 90: 2.88min\n",
      "Epoch [91/200], Training accuracy: 0.749, Validation accuracy: 0.741, loss = 1.552\n",
      "Time spent on epoch 91: 2.88min\n",
      "Epoch [92/200], Training accuracy: 0.752, Validation accuracy: 0.741, loss = 1.869\n",
      "Time spent on epoch 92: 2.87min\n",
      "Epoch [93/200], Training accuracy: 0.741, Validation accuracy: 0.726, loss = 1.837\n",
      "Time spent on epoch 93: 2.88min\n",
      "Epoch [94/200], Training accuracy: 0.763, Validation accuracy: 0.749, loss = 1.612\n",
      "Time spent on epoch 94: 2.98min\n"
     ]
    }
   ],
   "source": [
    "# repetimos 3 veces el experimento \n",
    "\n",
    "model1 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[3,4,6,3], widths=[64, 128, 256,512]).to(device)\n",
    "model2 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[3,4,6,3], widths=[64, 128, 256,512]).to(device)\n",
    "model3 = ResNet(in_channels=3, n_classes = 10, stem_features=64, depths=[3,4,6,3], widths=[64, 128, 256,512]).to(device)\n",
    "\n",
    "model1, training1, validation1, test1, loss1 = entrenamiento(model1, 200)\n",
    "model2, training2, validation2, test2, loss2 = entrenamiento(model2, 200)\n",
    "model3, training3, validation3, test3, loss3 = entrenamiento(model3, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d82eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardar resultados\n",
    "results_dict1 = {\"loss\": loss1,\n",
    "    'Train':training1,\n",
    "     'Validation': validation1,\n",
    "     \"Test\":test1}\n",
    "results_dict2 = {\"loss\": loss2,\n",
    "    'Train':training2,\n",
    "     'Validation': validation2,\n",
    "     \"Test\":test2}\n",
    "results_dict3 = {\"loss\": loss3,\n",
    "    'Train':training3,\n",
    "     'Validation': validation3,\n",
    "     \"Test\":test3}\n",
    "\n",
    "results1_base = pd.DataFrame(results_dict1)\n",
    "results2_base = pd.DataFrame(results_dict2)\n",
    "results3_base = pd.DataFrame(results_dict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da4806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (results1_base[\"Test\"].max() + results2_base[\"Test\"].max() + results3_base[\"Test\"].max())/3\n",
    "print(f\"Accuracy del modelo base: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f583df",
   "metadata": {},
   "outputs": [],
   "source": [
    "results1_base.to_csv(\"./results/results_convnext_base_1.csv\",index=False)\n",
    "results2_base.to_csv(\"./results/results_convnext_base_2.csv\",index=False)\n",
    "results3_base.to_csv(\"./results/results_convnext_base_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97481aed",
   "metadata": {},
   "source": [
    "% Changing stage compute ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3f1ce",
   "metadata": {},
   "source": [
    "% Changing stem to patchify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ae242",
   "metadata": {},
   "source": [
    "% resnextify y aumento de canales originalmente de 64 a 96, tambien hacer el depthwise en el bloque de bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7051347",
   "metadata": {},
   "source": [
    "% inverted bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20e5568",
   "metadata": {},
   "source": [
    "% Large Kernel Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482e116",
   "metadata": {},
   "source": [
    "% Micro Design"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
