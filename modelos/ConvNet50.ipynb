{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gg8mfplfyOtu"
   },
   "source": [
    "# Modelo de red neuronal convolucional base (18 capas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicar lo que vamos a hacer en este notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfgy16RnyScw"
   },
   "source": [
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "azUBxbX3x1Es"
   },
   "outputs": [],
   "source": [
    "# Importamos las paqueterías necesarias para el notebook\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# De ser posible utilizaremos GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7VGuEmJx3vD",
    "outputId": "8d2040df-f868-4890-9857-3d06a3f962f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def data_loader(data_dir,\n",
    "                batch_size,\n",
    "                random_seed=42,\n",
    "                valid_size=0.1,\n",
    "                shuffle=True,\n",
    "                test=False):\n",
    "    \"\"\"\n",
    "    Función para cargar los datos de CIFAR-10\n",
    "    \"\"\"\n",
    "    \n",
    "    # Definimos el transform para normalizar los datos con pytorch\n",
    "    # Los valores fueron obtenidos en el notebook de datos \"data_extraction.ipynb\"\n",
    "    normalize = transforms.Normalize(  \n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # Definimos el transform para preporcesar los datos\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    # Obtener los datos del conjunto de prueba\n",
    "    if test:\n",
    "        dataset = datasets.CIFAR10(\n",
    "          root=data_dir, train=False,\n",
    "          download=True, transform=transform_test,\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    # Cargamos una copia de los datos de entrenamiento\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform_train,\n",
    "    )\n",
    "    \n",
    "    # Cargamos una copia extra de los datos de entrenamiento para dividirlo después en el conjunto de validación\n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform_train,\n",
    "    )\n",
    "    \n",
    "    # Separamos los datos de entrenamiento y validación mediante índices\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    # Finalmente, definimos los conjuntos de entrenamiento y validación\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "\n",
    "# Aplicamos la función para cargar los datos de CIFAR-10, los guardamos en el directorio actual\n",
    "train_loader, valid_loader = data_loader(data_dir='./data',\n",
    "                                         batch_size=128)\n",
    "\n",
    "test_loader = data_loader(data_dir='./data',\n",
    "                              batch_size=128,\n",
    "                              test=True)    \n",
    "cifar10_classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mj9VrYQGybId"
   },
   "source": [
    "### Definición del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicar en qué consiste el modelo y por qué lo elegí de 18 capas y como lo vamos a testear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jf7woEnkx63E"
   },
   "outputs": [],
   "source": [
    "class ConvNet_50_capas(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo de red neuronal convolucional de 18 capas para clasificar imágenes en 10 clases posibles\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "                                  nn.BatchNorm2d(16),\n",
    "                                  nn.ReLU())\n",
    "        \n",
    "        self.bloc1 = nn.Sequential(nn.Conv2d(16, 16, kernel_size=3, stride = 1, padding = 1),\n",
    "                                   nn.BatchNorm2d(16),\n",
    "                                   nn.ReLU()\n",
    "                                   )\n",
    "        self.conv_down2 = nn.Sequential(nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  #  feature map size = (16,16)\n",
    "                                        nn.BatchNorm2d(32),\n",
    "                                        nn.ReLU()\n",
    "                                       )\n",
    "        \n",
    "        self.bloc2 = nn.Sequential(nn.Conv2d(32, 32, kernel_size=3, stride = 1, padding = 1),\n",
    "                                   nn.BatchNorm2d(32),\n",
    "                                   nn.ReLU()\n",
    "                                  )\n",
    "        \n",
    "        self.conv_down3 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  #  feature map size = (8,8)\n",
    "                                        nn.BatchNorm2d(64),\n",
    "                                        nn.ReLU()\n",
    "                                       )\n",
    "        \n",
    "        self.bloc3 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride = 1, padding = 1),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.ReLU()\n",
    "                                  )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64,10)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1(x)\n",
    "        for i in range(16):\n",
    "            out = self.bloc1(out)\n",
    "        out = self.conv_down2(out)\n",
    "        for i in range(15):\n",
    "            out = self.bloc2(out)\n",
    "        out = self.conv_down3(out)\n",
    "        for i in range(15):\n",
    "            out = self.bloc3(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = F.softmax(out, dim = 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9upO6XsyiJq"
   },
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-0-wgb7XyAgB"
   },
   "outputs": [],
   "source": [
    "# variables para guardar los resultados\n",
    "accuracy_training_epochs = []\n",
    "accuracy_validation_epochs = []\n",
    "loss_epoch = []\n",
    "test_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "alY6m1H7yl3e"
   },
   "outputs": [],
   "source": [
    "# hiperparámetros reportados en el artículo, número de épocas reducido.\n",
    "num_classes = 10\n",
    "num_epochs = 90\n",
    "\n",
    "model = ConvNet_50_capas().to(device)\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "        params=model.parameters(),\n",
    "        lr=0.1,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TwftzwYeyo6k",
    "outputId": "6609e387-28a2-492f-96ce-3e5182aca758"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/90], Training accuracy: 0.195, Validation accuracy: 0.192, loss = 2.251\n",
      "Time spent on epoch 1: 1.68min\n",
      "Epoch [2/90], Training accuracy: 0.164, Validation accuracy: 0.161, loss = 2.238\n",
      "Time spent on epoch 2: 1.72min\n",
      "Epoch [3/90], Training accuracy: 0.24, Validation accuracy: 0.241, loss = 2.222\n",
      "Time spent on epoch 3: 1.87min\n",
      "Epoch [4/90], Training accuracy: 0.244, Validation accuracy: 0.245, loss = 2.163\n",
      "Time spent on epoch 4: 1.79min\n",
      "Epoch [5/90], Training accuracy: 0.249, Validation accuracy: 0.24, loss = 2.14\n",
      "Time spent on epoch 5: 1.83min\n",
      "Epoch [6/90], Training accuracy: 0.255, Validation accuracy: 0.244, loss = 2.25\n",
      "Time spent on epoch 6: 1.87min\n",
      "Epoch [7/90], Training accuracy: 0.266, Validation accuracy: 0.261, loss = 2.175\n",
      "Time spent on epoch 7: 1.91min\n",
      "Epoch [8/90], Training accuracy: 0.259, Validation accuracy: 0.258, loss = 2.214\n",
      "Time spent on epoch 8: 1.93min\n",
      "Epoch [9/90], Training accuracy: 0.266, Validation accuracy: 0.253, loss = 2.203\n",
      "Time spent on epoch 9: 1.96min\n",
      "Epoch [10/90], Training accuracy: 0.273, Validation accuracy: 0.272, loss = 2.111\n",
      "Time spent on epoch 10: 1.97min\n",
      "Epoch [11/90], Training accuracy: 0.25, Validation accuracy: 0.248, loss = 2.186\n",
      "Time spent on epoch 11: 1.98min\n",
      "Epoch [12/90], Training accuracy: 0.296, Validation accuracy: 0.291, loss = 2.119\n",
      "Time spent on epoch 12: 1.99min\n",
      "Epoch [13/90], Training accuracy: 0.294, Validation accuracy: 0.291, loss = 2.019\n",
      "Time spent on epoch 13: 1.99min\n",
      "Epoch [14/90], Training accuracy: 0.305, Validation accuracy: 0.294, loss = 2.072\n",
      "Time spent on epoch 14: 1.98min\n",
      "Epoch [15/90], Training accuracy: 0.309, Validation accuracy: 0.303, loss = 2.181\n",
      "Time spent on epoch 15: 1.99min\n",
      "Epoch [16/90], Training accuracy: 0.327, Validation accuracy: 0.321, loss = 2.122\n",
      "Time spent on epoch 16: 1.99min\n",
      "Epoch [17/90], Training accuracy: 0.34, Validation accuracy: 0.334, loss = 2.115\n",
      "Time spent on epoch 17: 2.0min\n",
      "Epoch [18/90], Training accuracy: 0.345, Validation accuracy: 0.339, loss = 2.108\n",
      "Time spent on epoch 18: 2.0min\n",
      "Epoch [19/90], Training accuracy: 0.359, Validation accuracy: 0.342, loss = 2.137\n",
      "Time spent on epoch 19: 1.99min\n",
      "Epoch [20/90], Training accuracy: 0.385, Validation accuracy: 0.38, loss = 2.079\n",
      "Time spent on epoch 20: 2.01min\n",
      "Epoch [21/90], Training accuracy: 0.385, Validation accuracy: 0.374, loss = 2.076\n",
      "Time spent on epoch 21: 2.0min\n",
      "Epoch [22/90], Training accuracy: 0.389, Validation accuracy: 0.381, loss = 2.078\n",
      "Time spent on epoch 22: 2.01min\n",
      "Epoch [23/90], Training accuracy: 0.395, Validation accuracy: 0.379, loss = 1.989\n",
      "Time spent on epoch 23: 2.02min\n",
      "Epoch [24/90], Training accuracy: 0.401, Validation accuracy: 0.391, loss = 2.071\n",
      "Time spent on epoch 24: 2.01min\n",
      "Epoch [25/90], Training accuracy: 0.402, Validation accuracy: 0.404, loss = 2.09\n",
      "Time spent on epoch 25: 2.02min\n",
      "Epoch [26/90], Training accuracy: 0.423, Validation accuracy: 0.411, loss = 2.061\n",
      "Time spent on epoch 26: 2.0min\n",
      "Epoch [27/90], Training accuracy: 0.434, Validation accuracy: 0.427, loss = 2.024\n",
      "Time spent on epoch 27: 2.04min\n",
      "Epoch [28/90], Training accuracy: 0.43, Validation accuracy: 0.423, loss = 2.026\n",
      "Time spent on epoch 28: 2.02min\n",
      "Epoch [29/90], Training accuracy: 0.444, Validation accuracy: 0.425, loss = 2.012\n",
      "Time spent on epoch 29: 2.01min\n",
      "Epoch [30/90], Training accuracy: 0.443, Validation accuracy: 0.428, loss = 1.951\n",
      "Time spent on epoch 30: 2.02min\n",
      "Epoch [31/90], Training accuracy: 0.451, Validation accuracy: 0.442, loss = 2.033\n",
      "Time spent on epoch 31: 2.02min\n",
      "Epoch [32/90], Training accuracy: 0.456, Validation accuracy: 0.448, loss = 2.028\n",
      "Time spent on epoch 32: 2.01min\n",
      "Epoch [33/90], Training accuracy: 0.457, Validation accuracy: 0.445, loss = 2.041\n",
      "Time spent on epoch 33: 2.01min\n",
      "Epoch [34/90], Training accuracy: 0.482, Validation accuracy: 0.468, loss = 1.947\n",
      "Time spent on epoch 34: 2.02min\n",
      "Epoch [35/90], Training accuracy: 0.485, Validation accuracy: 0.478, loss = 1.976\n",
      "Time spent on epoch 35: 2.01min\n",
      "Epoch [36/90], Training accuracy: 0.484, Validation accuracy: 0.471, loss = 1.91\n",
      "Time spent on epoch 36: 2.01min\n",
      "Epoch [37/90], Training accuracy: 0.481, Validation accuracy: 0.478, loss = 2.026\n",
      "Time spent on epoch 37: 2.02min\n",
      "Epoch [38/90], Training accuracy: 0.448, Validation accuracy: 0.428, loss = 1.984\n",
      "Time spent on epoch 38: 2.01min\n",
      "Epoch [39/90], Training accuracy: 0.512, Validation accuracy: 0.506, loss = 2.0\n",
      "Time spent on epoch 39: 2.01min\n",
      "Epoch [40/90], Training accuracy: 0.506, Validation accuracy: 0.497, loss = 1.855\n",
      "Time spent on epoch 40: 2.01min\n",
      "Epoch [41/90], Training accuracy: 0.511, Validation accuracy: 0.502, loss = 1.931\n",
      "Time spent on epoch 41: 2.02min\n",
      "Epoch [42/90], Training accuracy: 0.492, Validation accuracy: 0.483, loss = 2.0\n",
      "Time spent on epoch 42: 2.01min\n",
      "Epoch [43/90], Training accuracy: 0.509, Validation accuracy: 0.5, loss = 2.007\n",
      "Time spent on epoch 43: 2.0min\n",
      "Epoch [44/90], Training accuracy: 0.523, Validation accuracy: 0.515, loss = 1.757\n",
      "Time spent on epoch 44: 2.01min\n",
      "Epoch [45/90], Training accuracy: 0.528, Validation accuracy: 0.517, loss = 1.895\n",
      "Time spent on epoch 45: 2.01min\n",
      "Epoch [46/90], Training accuracy: 0.531, Validation accuracy: 0.526, loss = 2.004\n",
      "Time spent on epoch 46: 2.0min\n",
      "Epoch [47/90], Training accuracy: 0.527, Validation accuracy: 0.512, loss = 2.004\n",
      "Time spent on epoch 47: 2.0min\n",
      "Epoch [48/90], Training accuracy: 0.541, Validation accuracy: 0.527, loss = 1.928\n",
      "Time spent on epoch 48: 2.03min\n",
      "Epoch [49/90], Training accuracy: 0.538, Validation accuracy: 0.541, loss = 1.868\n",
      "Time spent on epoch 49: 2.01min\n",
      "Epoch [50/90], Training accuracy: 0.538, Validation accuracy: 0.524, loss = 1.972\n",
      "Time spent on epoch 50: 2.03min\n",
      "Epoch [51/90], Training accuracy: 0.543, Validation accuracy: 0.529, loss = 1.954\n",
      "Time spent on epoch 51: 2.02min\n",
      "Epoch [52/90], Training accuracy: 0.55, Validation accuracy: 0.535, loss = 1.995\n",
      "Time spent on epoch 52: 2.02min\n",
      "Epoch [53/90], Training accuracy: 0.542, Validation accuracy: 0.534, loss = 1.912\n",
      "Time spent on epoch 53: 2.02min\n",
      "Epoch [54/90], Training accuracy: 0.554, Validation accuracy: 0.541, loss = 1.829\n",
      "Time spent on epoch 54: 2.02min\n",
      "Epoch [55/90], Training accuracy: 0.556, Validation accuracy: 0.556, loss = 1.951\n",
      "Time spent on epoch 55: 2.02min\n",
      "Epoch [56/90], Training accuracy: 0.557, Validation accuracy: 0.544, loss = 1.893\n",
      "Time spent on epoch 56: 2.04min\n",
      "Epoch [57/90], Training accuracy: 0.567, Validation accuracy: 0.553, loss = 1.98\n",
      "Time spent on epoch 57: 2.0min\n",
      "Epoch [58/90], Training accuracy: 0.55, Validation accuracy: 0.539, loss = 1.912\n",
      "Time spent on epoch 58: 2.01min\n",
      "Epoch [59/90], Training accuracy: 0.566, Validation accuracy: 0.556, loss = 1.765\n",
      "Time spent on epoch 59: 2.02min\n",
      "Epoch [60/90], Training accuracy: 0.57, Validation accuracy: 0.562, loss = 1.881\n",
      "Time spent on epoch 60: 2.03min\n",
      "Epoch [61/90], Training accuracy: 0.582, Validation accuracy: 0.568, loss = 1.923\n",
      "Time spent on epoch 61: 2.01min\n",
      "Epoch [62/90], Training accuracy: 0.584, Validation accuracy: 0.573, loss = 1.874\n",
      "Time spent on epoch 62: 2.01min\n",
      "Epoch [63/90], Training accuracy: 0.588, Validation accuracy: 0.577, loss = 1.894\n",
      "Time spent on epoch 63: 2.01min\n",
      "Epoch [64/90], Training accuracy: 0.59, Validation accuracy: 0.582, loss = 1.821\n",
      "Time spent on epoch 64: 2.02min\n",
      "Epoch [65/90], Training accuracy: 0.59, Validation accuracy: 0.576, loss = 1.843\n",
      "Time spent on epoch 65: 2.02min\n",
      "Epoch [66/90], Training accuracy: 0.593, Validation accuracy: 0.581, loss = 1.911\n",
      "Time spent on epoch 66: 2.0min\n",
      "Epoch [67/90], Training accuracy: 0.594, Validation accuracy: 0.593, loss = 1.852\n",
      "Time spent on epoch 67: 2.02min\n",
      "Epoch [68/90], Training accuracy: 0.593, Validation accuracy: 0.585, loss = 1.836\n",
      "Time spent on epoch 68: 2.02min\n",
      "Epoch [69/90], Training accuracy: 0.599, Validation accuracy: 0.598, loss = 1.745\n",
      "Time spent on epoch 69: 2.0min\n",
      "Epoch [70/90], Training accuracy: 0.603, Validation accuracy: 0.594, loss = 1.906\n",
      "Time spent on epoch 70: 2.01min\n",
      "Epoch [71/90], Training accuracy: 0.609, Validation accuracy: 0.599, loss = 1.855\n",
      "Time spent on epoch 71: 2.01min\n",
      "Epoch [72/90], Training accuracy: 0.609, Validation accuracy: 0.607, loss = 1.921\n",
      "Time spent on epoch 72: 2.01min\n",
      "Epoch [73/90], Training accuracy: 0.616, Validation accuracy: 0.604, loss = 1.804\n",
      "Time spent on epoch 73: 2.03min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/90], Training accuracy: 0.617, Validation accuracy: 0.604, loss = 1.801\n",
      "Time spent on epoch 74: 2.02min\n",
      "Epoch [75/90], Training accuracy: 0.621, Validation accuracy: 0.604, loss = 1.887\n",
      "Time spent on epoch 75: 2.0min\n",
      "Epoch [76/90], Training accuracy: 0.62, Validation accuracy: 0.605, loss = 1.85\n",
      "Time spent on epoch 76: 2.02min\n",
      "Epoch [77/90], Training accuracy: 0.624, Validation accuracy: 0.61, loss = 1.831\n",
      "Time spent on epoch 77: 2.01min\n",
      "Epoch [78/90], Training accuracy: 0.625, Validation accuracy: 0.603, loss = 1.905\n",
      "Time spent on epoch 78: 2.01min\n",
      "Epoch [79/90], Training accuracy: 0.626, Validation accuracy: 0.619, loss = 1.814\n",
      "Time spent on epoch 79: 2.01min\n",
      "Epoch [80/90], Training accuracy: 0.631, Validation accuracy: 0.611, loss = 1.819\n",
      "Time spent on epoch 80: 2.01min\n",
      "Epoch [81/90], Training accuracy: 0.632, Validation accuracy: 0.617, loss = 1.794\n",
      "Time spent on epoch 81: 2.01min\n",
      "Epoch [82/90], Training accuracy: 0.632, Validation accuracy: 0.616, loss = 1.957\n",
      "Time spent on epoch 82: 2.02min\n",
      "Epoch [83/90], Training accuracy: 0.634, Validation accuracy: 0.61, loss = 1.812\n",
      "Time spent on epoch 83: 2.01min\n",
      "Epoch [84/90], Training accuracy: 0.634, Validation accuracy: 0.615, loss = 1.909\n",
      "Time spent on epoch 84: 2.0min\n",
      "Epoch [85/90], Training accuracy: 0.635, Validation accuracy: 0.616, loss = 1.882\n",
      "Time spent on epoch 85: 2.02min\n",
      "Epoch [86/90], Training accuracy: 0.636, Validation accuracy: 0.619, loss = 1.787\n",
      "Time spent on epoch 86: 2.03min\n",
      "Epoch [87/90], Training accuracy: 0.636, Validation accuracy: 0.617, loss = 1.741\n",
      "Time spent on epoch 87: 2.03min\n",
      "Epoch [88/90], Training accuracy: 0.637, Validation accuracy: 0.62, loss = 1.795\n",
      "Time spent on epoch 88: 2.01min\n",
      "Epoch [89/90], Training accuracy: 0.637, Validation accuracy: 0.617, loss = 1.796\n",
      "Time spent on epoch 89: 2.0min\n",
      "Epoch [90/90], Training accuracy: 0.64, Validation accuracy: 0.623, loss = 1.75\n",
      "Time spent on epoch 90: 2.01min\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Mover a los tensores a GPU de ser posible\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass y descenso de gradiente estocástico\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Ahorro de memoria\n",
    "        del images, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    loss_epoch.append(loss.item())\n",
    "    lr_scheduler.step() # Implementación de learning rate decay\n",
    "\n",
    "    # Exactitud en el conjunto de validación\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "        val_accuracy = correct/total\n",
    "        accuracy_validation_epochs.append(val_accuracy)\n",
    "\n",
    "    # Exactitud en el total del conjunto de entrenamiento\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "        train_accuracy = correct/total\n",
    "        accuracy_training_epochs.append(train_accuracy)\n",
    "\n",
    "    # Exactitud en el conjunto de prueba\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "        t_acc = correct/total\n",
    "        test_accuracy.append(t_acc)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training accuracy: {round(train_accuracy,3)}, Validation accuracy: {round(val_accuracy,3)}, loss = {round(loss_epoch[-1],3)}\")\n",
    "    print(f\"Time spent on epoch {epoch+1}: {round((time.time()-start_time)/60,2)}min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ILjQlK7S0KIc"
   },
   "outputs": [],
   "source": [
    "# guardar el modelo\n",
    "torch.save(model,\"./convnet50.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9l0N0Cc6kyrO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kosoi\\AppData\\Local\\Temp\\ipykernel_17680\\4080736814.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YbZ8AZVHgz9a"
   },
   "outputs": [],
   "source": [
    "results_dict = {\"loss\": loss_epoch,\n",
    "    'Train':accuracy_training_epochs,\n",
    "     'Validation': accuracy_validation_epochs,\n",
    "     \"Test\":test_accuracy}\n",
    "results = pd.DataFrame(results_dict)\n",
    "results.to_csv(\"./results/results_convnet50.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "9DFGffViv61T",
    "outputId": "b999d015-d538-4bd5-b130-70620a0aefe1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>Train</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.251232</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>0.1924</td>\n",
       "      <td>0.1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.238303</td>\n",
       "      <td>0.164311</td>\n",
       "      <td>0.1610</td>\n",
       "      <td>0.1725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.221881</td>\n",
       "      <td>0.240044</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.2483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.163149</td>\n",
       "      <td>0.244156</td>\n",
       "      <td>0.2448</td>\n",
       "      <td>0.2596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.139840</td>\n",
       "      <td>0.248556</td>\n",
       "      <td>0.2402</td>\n",
       "      <td>0.2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1.786909</td>\n",
       "      <td>0.635600</td>\n",
       "      <td>0.6186</td>\n",
       "      <td>0.6275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.741324</td>\n",
       "      <td>0.636289</td>\n",
       "      <td>0.6168</td>\n",
       "      <td>0.6313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.795027</td>\n",
       "      <td>0.636733</td>\n",
       "      <td>0.6196</td>\n",
       "      <td>0.6299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.795526</td>\n",
       "      <td>0.636822</td>\n",
       "      <td>0.6174</td>\n",
       "      <td>0.6272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.749916</td>\n",
       "      <td>0.639978</td>\n",
       "      <td>0.6230</td>\n",
       "      <td>0.6307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss     Train  Validation    Test\n",
       "0   2.251232  0.194800      0.1924  0.1975\n",
       "1   2.238303  0.164311      0.1610  0.1725\n",
       "2   2.221881  0.240044      0.2410  0.2483\n",
       "3   2.163149  0.244156      0.2448  0.2596\n",
       "4   2.139840  0.248556      0.2402  0.2600\n",
       "..       ...       ...         ...     ...\n",
       "85  1.786909  0.635600      0.6186  0.6275\n",
       "86  1.741324  0.636289      0.6168  0.6313\n",
       "87  1.795027  0.636733      0.6196  0.6299\n",
       "88  1.795526  0.636822      0.6174  0.6272\n",
       "89  1.749916  0.639978      0.6230  0.6307\n",
       "\n",
       "[90 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "st0WCjLFv8hw"
   },
   "outputs": [],
   "source": [
    "accuracy = max(results[\"Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6313\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
